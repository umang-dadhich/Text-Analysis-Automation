{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb37fe6-8b4e-4719-925a-3b05955e802d",
   "metadata": {},
   "source": [
    "# Text Analysis Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdddf518-ee3d-4a66-8e98-14f89b84e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b4e004-0986-4489-929d-e5ceb0042f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb50c3d-9f62-4448-ae94-9a67d74bb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce40a3f-d3f2-4f67-818c-2bf4131422ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input Excel file\n",
    "df = pd.read_excel(r\"C:\\Users\\admin\\Downloads\\input.xlsx\")  # Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69b8535-8361-4814-9734-287f6bdbf363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450c37c1-8cd5-47fb-a842-139763d63525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880b7cf7-3a87-4929-9c13-c78129c05da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Set up a requests session with dynamic User-Agent\n",
    "session = requests.Session()\n",
    "ua = UserAgent()\n",
    "session.headers.update({\"User-Agent\": ua.random})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff07aa-bd42-4c02-af05-bdac1390b3c6",
   "metadata": {},
   "source": [
    "#  Extracted_Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08a77dd-d6eb-40a7-b9bb-5357d97c41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Extracted_Articles\\Netclan20241017.txt\n",
      "Saved: Extracted_Articles\\Netclan20241018.txt\n",
      "Saved: Extracted_Articles\\Netclan20241019.txt\n",
      "Saved: Extracted_Articles\\Netclan20241020.txt\n",
      "Saved: Extracted_Articles\\Netclan20241021.txt\n",
      "Saved: Extracted_Articles\\Netclan20241022.txt\n",
      "Saved: Extracted_Articles\\Netclan20241023.txt\n",
      "Saved: Extracted_Articles\\Netclan20241024.txt\n",
      "Saved: Extracted_Articles\\Netclan20241025.txt\n",
      "Saved: Extracted_Articles\\Netclan20241026.txt\n",
      "Saved: Extracted_Articles\\Netclan20241027.txt\n",
      "Saved: Extracted_Articles\\Netclan20241028.txt\n",
      "Saved: Extracted_Articles\\Netclan20241029.txt\n",
      "Saved: Extracted_Articles\\Netclan20241030.txt\n",
      "Saved: Extracted_Articles\\Netclan20241031.txt\n",
      "Saved: Extracted_Articles\\Netclan20241032.txt\n",
      "Saved: Extracted_Articles\\Netclan20241033.txt\n",
      "Saved: Extracted_Articles\\Netclan20241034.txt\n",
      "Saved: Extracted_Articles\\Netclan20241035.txt\n",
      "Saved: Extracted_Articles\\Netclan20241036.txt\n",
      "Saved: Extracted_Articles\\Netclan20241037.txt\n",
      "Saved: Extracted_Articles\\Netclan20241038.txt\n",
      "Saved: Extracted_Articles\\Netclan20241039.txt\n",
      "Saved: Extracted_Articles\\Netclan20241040.txt\n",
      "Saved: Extracted_Articles\\Netclan20241041.txt\n",
      "Saved: Extracted_Articles\\Netclan20241042.txt\n",
      "Saved: Extracted_Articles\\Netclan20241043.txt\n",
      "Saved: Extracted_Articles\\Netclan20241044.txt\n",
      "Saved: Extracted_Articles\\Netclan20241045.txt\n",
      "Saved: Extracted_Articles\\Netclan20241046.txt\n",
      "Saved: Extracted_Articles\\Netclan20241047.txt\n",
      "Saved: Extracted_Articles\\Netclan20241048.txt\n",
      "Saved: Extracted_Articles\\Netclan20241049.txt\n",
      "Saved: Extracted_Articles\\Netclan20241050.txt\n",
      "Saved: Extracted_Articles\\Netclan20241051.txt\n",
      "Saved: Extracted_Articles\\Netclan20241052.txt\n",
      "Saved: Extracted_Articles\\Netclan20241053.txt\n",
      "Saved: Extracted_Articles\\Netclan20241054.txt\n",
      "Saved: Extracted_Articles\\Netclan20241055.txt\n",
      "Saved: Extracted_Articles\\Netclan20241056.txt\n",
      "Saved: Extracted_Articles\\Netclan20241057.txt\n",
      "Saved: Extracted_Articles\\Netclan20241058.txt\n",
      "Saved: Extracted_Articles\\Netclan20241059.txt\n",
      "Saved: Extracted_Articles\\Netclan20241060.txt\n",
      "Saved: Extracted_Articles\\Netclan20241061.txt\n",
      "Saved: Extracted_Articles\\Netclan20241062.txt\n",
      "Saved: Extracted_Articles\\Netclan20241063.txt\n",
      "Saved: Extracted_Articles\\Netclan20241064.txt\n",
      "Saved: Extracted_Articles\\Netclan20241065.txt\n",
      "Saved: Extracted_Articles\\Netclan20241066.txt\n",
      "Saved: Extracted_Articles\\Netclan20241067.txt\n",
      "Saved: Extracted_Articles\\Netclan20241068.txt\n",
      "Saved: Extracted_Articles\\Netclan20241069.txt\n",
      "Saved: Extracted_Articles\\Netclan20241070.txt\n",
      "Saved: Extracted_Articles\\Netclan20241071.txt\n",
      "Saved: Extracted_Articles\\Netclan20241072.txt\n",
      "Saved: Extracted_Articles\\Netclan20241073.txt\n",
      "Saved: Extracted_Articles\\Netclan20241074.txt\n",
      "Saved: Extracted_Articles\\Netclan20241075.txt\n",
      "Saved: Extracted_Articles\\Netclan20241076.txt\n",
      "Saved: Extracted_Articles\\Netclan20241077.txt\n",
      "Saved: Extracted_Articles\\Netclan20241078.txt\n",
      "Saved: Extracted_Articles\\Netclan20241079.txt\n",
      "Saved: Extracted_Articles\\Netclan20241080.txt\n",
      "Saved: Extracted_Articles\\Netclan20241081.txt\n",
      "Saved: Extracted_Articles\\Netclan20241082.txt\n",
      "Saved: Extracted_Articles\\Netclan20241083.txt\n",
      "Saved: Extracted_Articles\\Netclan20241084.txt\n",
      "Saved: Extracted_Articles\\Netclan20241085.txt\n",
      "Saved: Extracted_Articles\\Netclan20241086.txt\n",
      "Saved: Extracted_Articles\\Netclan20241087.txt\n",
      "Saved: Extracted_Articles\\Netclan20241088.txt\n",
      "Saved: Extracted_Articles\\Netclan20241089.txt\n",
      "Saved: Extracted_Articles\\Netclan20241090.txt\n",
      "Saved: Extracted_Articles\\Netclan20241091.txt\n",
      "Saved: Extracted_Articles\\Netclan20241092.txt\n",
      "Saved: Extracted_Articles\\Netclan20241093.txt\n",
      "Saved: Extracted_Articles\\Netclan20241094.txt\n",
      "Saved: Extracted_Articles\\Netclan20241095.txt\n",
      "Saved: Extracted_Articles\\Netclan20241096.txt\n",
      "Saved: Extracted_Articles\\Netclan20241097.txt\n",
      "Saved: Extracted_Articles\\Netclan20241098.txt\n",
      "Saved: Extracted_Articles\\Netclan20241099.txt\n",
      "Saved: Extracted_Articles\\Netclan20241100.txt\n",
      "Saved: Extracted_Articles\\Netclan20241101.txt\n",
      "Saved: Extracted_Articles\\Netclan20241102.txt\n",
      "Saved: Extracted_Articles\\Netclan20241103.txt\n",
      "Saved: Extracted_Articles\\Netclan20241104.txt\n",
      "Saved: Extracted_Articles\\Netclan20241105.txt\n",
      "Saved: Extracted_Articles\\Netclan20241106.txt\n",
      "Saved: Extracted_Articles\\Netclan20241107.txt\n",
      "Saved: Extracted_Articles\\Netclan20241108.txt\n",
      "Saved: Extracted_Articles\\Netclan20241109.txt\n",
      "Saved: Extracted_Articles\\Netclan20241110.txt\n",
      "Saved: Extracted_Articles\\Netclan20241111.txt\n",
      "Saved: Extracted_Articles\\Netclan20241112.txt\n",
      "Saved: Extracted_Articles\\Netclan20241113.txt\n",
      "Saved: Extracted_Articles\\Netclan20241114.txt\n",
      "Saved: Extracted_Articles\\Netclan20241115.txt\n",
      "Saved: Extracted_Articles\\Netclan20241116.txt\n",
      "Saved: Extracted_Articles\\Netclan20241117.txt\n",
      "Saved: Extracted_Articles\\Netclan20241118.txt\n",
      "Saved: Extracted_Articles\\Netclan20241119.txt\n",
      "Saved: Extracted_Articles\\Netclan20241120.txt\n",
      "Saved: Extracted_Articles\\Netclan20241121.txt\n",
      "Saved: Extracted_Articles\\Netclan20241122.txt\n",
      "Saved: Extracted_Articles\\Netclan20241123.txt\n",
      "Saved: Extracted_Articles\\Netclan20241124.txt\n",
      "Saved: Extracted_Articles\\Netclan20241125.txt\n",
      "Saved: Extracted_Articles\\Netclan20241126.txt\n",
      "Saved: Extracted_Articles\\Netclan20241127.txt\n",
      "Saved: Extracted_Articles\\Netclan20241128.txt\n",
      "Saved: Extracted_Articles\\Netclan20241129.txt\n",
      "Saved: Extracted_Articles\\Netclan20241130.txt\n",
      "Saved: Extracted_Articles\\Netclan20241131.txt\n",
      "Saved: Extracted_Articles\\Netclan20241132.txt\n",
      "Saved: Extracted_Articles\\Netclan20241133.txt\n",
      "Saved: Extracted_Articles\\Netclan20241134.txt\n",
      "Saved: Extracted_Articles\\Netclan20241135.txt\n",
      "Saved: Extracted_Articles\\Netclan20241136.txt\n",
      "Saved: Extracted_Articles\\Netclan20241137.txt\n",
      "Saved: Extracted_Articles\\Netclan20241138.txt\n",
      "Saved: Extracted_Articles\\Netclan20241139.txt\n",
      "Saved: Extracted_Articles\\Netclan20241140.txt\n",
      "Saved: Extracted_Articles\\Netclan20241141.txt\n",
      "Saved: Extracted_Articles\\Netclan20241142.txt\n",
      "Saved: Extracted_Articles\\Netclan20241143.txt\n",
      "Saved: Extracted_Articles\\Netclan20241144.txt\n",
      "Saved: Extracted_Articles\\Netclan20241145.txt\n",
      "Saved: Extracted_Articles\\Netclan20241146.txt\n",
      "Saved: Extracted_Articles\\Netclan20241147.txt\n",
      "Saved: Extracted_Articles\\Netclan20241148.txt\n",
      "Saved: Extracted_Articles\\Netclan20241149.txt\n",
      "Saved: Extracted_Articles\\Netclan20241150.txt\n",
      "Saved: Extracted_Articles\\Netclan20241151.txt\n",
      "Saved: Extracted_Articles\\Netclan20241152.txt\n",
      "Saved: Extracted_Articles\\Netclan20241153.txt\n",
      "Saved: Extracted_Articles\\Netclan20241154.txt\n",
      "Saved: Extracted_Articles\\Netclan20241155.txt\n",
      "Saved: Extracted_Articles\\Netclan20241156.txt\n",
      "Saved: Extracted_Articles\\Netclan20241157.txt\n",
      "Saved: Extracted_Articles\\Netclan20241158.txt\n",
      "Saved: Extracted_Articles\\Netclan20241159.txt\n",
      "Saved: Extracted_Articles\\Netclan20241160.txt\n",
      "Saved: Extracted_Articles\\Netclan20241161.txt\n",
      "Saved: Extracted_Articles\\Netclan20241162.txt\n",
      "Saved: Extracted_Articles\\Netclan20241163.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Load the input Excel file\n",
    "file_path = \"/mnt/data/Input.xlsx\"\n",
    "df = pd.read_excel(r\"C:\\Users\\admin\\Downloads\\input.xlsx\")\n",
    "\n",
    "\n",
    "# Create a directory to save articles if not exists\n",
    "output_dir = \"Extracted_Articles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set up a requests session with dynamic User-Agent\n",
    "session = requests.Session()\n",
    "ua = UserAgent()\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extracts the title and text of an article from a given URL with retries.\"\"\"\n",
    "    try:\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            headers = {\"User-Agent\": ua.random}\n",
    "            response = session.get(url, headers=headers, timeout=15)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                break  # Exit loop if successful\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Access Denied (403) for {url}. The site may be blocking bots.\")\n",
    "                return None, None\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1} failed for {url}, retrying...\")\n",
    "                time.sleep(5)  # Longer wait time to avoid detection\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url} after 3 attempts\")\n",
    "            return None, None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
    "        \n",
    "        # Extract article text (from paragraphs)\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \"\\n\".join([para.get_text().strip() for para in paragraphs])\n",
    "        \n",
    "        return title, text if text else \"No content extracted\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error for {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    \n",
    "    title, content = extract_article_text(url)\n",
    "    \n",
    "    if title and content:\n",
    "        # Save as a text file\n",
    "        file_name = os.path.join(output_dir, f\"{url_id}.txt\")\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"Title: {title}\\n\\n\")\n",
    "            file.write(content)\n",
    "        print(f\"Saved: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping {url_id}, no content extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e0a2ed-8d55-4ebb-b9ca-a90919f6086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textstat) (76.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cmudict->textstat) (7.0.1)\n",
      "Collecting importlib-resources>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.17.0)\n",
      "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.4 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/939.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 939.4/939.4 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pyphen, importlib-resources, cmudict, textstat\n",
      "Successfully installed cmudict-1.0.32 importlib-resources-6.5.2 pyphen-0.17.2 textstat-0.7.5\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa11c69-da8f-4d4a-8b25-a288392a9838",
   "metadata": {},
   "source": [
    "# Perform text analysis and return computed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7f99ac-6db2-4416-b847-3dc95b4db100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Perform text analysis and return computed variables.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aedeadb-d599-4865-8e70-34e4a4f3753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.12.14)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c695c9-f0e8-47b7-ae73-f67dd1a24ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/624.3 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 624.3/624.3 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de08a72-e16d-4362-86d8-c1f544187422",
   "metadata": {},
   "source": [
    "# sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6df9b18a-44e8-4059-b704-72ef1929eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 0.676\n",
      "Negative Score: 0.0\n",
      "Polarity Score: 0.9259\n",
      "Subjectivity Score: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample text\n",
    "text = \"I love this product! It's amazing and works perfectly.\"\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment = analyzer.polarity_scores(text)\n",
    "positive_score = sentiment['pos']\n",
    "negative_score = sentiment['neg']\n",
    "polarity_score = sentiment['compound']\n",
    "subjectivity_score = TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Print results\n",
    "print(\"Positive Score:\", positive_score)\n",
    "print(\"Negative Score:\", negative_score)\n",
    "print(\"Polarity Score:\", polarity_score)\n",
    "print(\"Subjectivity Score:\", subjectivity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c571b8f-3094-4aaf-8776-733ade7bdb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: nltk 3.9.1\n",
      "Uninstalling nltk-3.9.1:\n",
      "  Successfully uninstalled nltk-3.9.1\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall nltk -y\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a47bad3-e7ab-4412-b340-69860536c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116614b4-3dfa-457f-ab16-7427af0420ae",
   "metadata": {},
   "source": [
    "# readability_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86738477-f3d1-4b3b-951a-8ccb08c08f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_count': 12, 'sentence_count': 2, 'avg_sentence_length': 6.0, 'complex_word_count': 3, 'percentage_complex_words': 25.0, 'fog_index': 12.4, 'syllables_per_word': 1.8333333333333333}\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "def readability_analysis(text):\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "\n",
    "    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) > 2)\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count else 0\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    syllables_per_word = textstat.syllable_count(text) / word_count if word_count else 0\n",
    "\n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"complex_word_count\": complex_word_count,\n",
    "        \"percentage_complex_words\": percentage_complex_words,\n",
    "        \"fog_index\": fog_index,\n",
    "        \"syllables_per_word\": syllables_per_word\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence. It will be used for readability analysis!\"\n",
    "result = readability_analysis(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "346461bf-8296-4e1b-a461-47213b5c1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (76.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.8 MB 5.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/11.8 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/11.8 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/11.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/11.8 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.3 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 4.8 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 5.3 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 10.0/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf78b82-bd5c-4ced-8354-239139f9bb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 6.0, 25.0, 12.4, 6.0, 3, 12, 1.8333333333333333, 1, 4.916666666666667]\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load SpaCy's English model\n",
    "\n",
    "def readability_analysis(text):\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "\n",
    "    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) > 2)\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count else 0\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    syllables_per_word = textstat.syllable_count(text) / word_count if word_count else 0\n",
    "\n",
    "    doc = nlp(text)\n",
    "    personal_pronouns = sum(1 for token in doc if token.tag_ == 'PRP')\n",
    "\n",
    "    # Placeholder values for sentiment analysis scores (if applicable)\n",
    "    positive_score = 0  \n",
    "    negative_score = 0  \n",
    "    polarity_score = 0  \n",
    "    subjectivity_score = 0  \n",
    "\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
    "\n",
    "    return [\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "        avg_sentence_length, percentage_complex_words, fog_index,\n",
    "        avg_sentence_length, complex_word_count, word_count, syllables_per_word,\n",
    "        personal_pronouns, avg_word_length\n",
    "    ]\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence. It will be used for readability analysis!\"\n",
    "result = readability_analysis(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612b52bb-2608-4049-9ecc-aa8bba31af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize output data\n",
    "output_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45c48d-5a92-4c02-804e-a874c0812a94",
   "metadata": {},
   "source": [
    "# Output Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37cb764f-ffc0-4813-bf80-3df669dab6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: output_texts\\Netclan20241017.txt\n",
      "Saved article: output_texts\\Netclan20241018.txt\n",
      "Saved article: output_texts\\Netclan20241019.txt\n",
      "Saved article: output_texts\\Netclan20241020.txt\n",
      "Saved article: output_texts\\Netclan20241021.txt\n",
      "Saved article: output_texts\\Netclan20241022.txt\n",
      "Saved article: output_texts\\Netclan20241023.txt\n",
      "Saved article: output_texts\\Netclan20241024.txt\n",
      "Saved article: output_texts\\Netclan20241025.txt\n",
      "Saved article: output_texts\\Netclan20241026.txt\n",
      "Saved article: output_texts\\Netclan20241027.txt\n",
      "Saved article: output_texts\\Netclan20241028.txt\n",
      "Saved article: output_texts\\Netclan20241029.txt\n",
      "Saved article: output_texts\\Netclan20241030.txt\n",
      "Saved article: output_texts\\Netclan20241031.txt\n",
      "Saved article: output_texts\\Netclan20241032.txt\n",
      "Saved article: output_texts\\Netclan20241033.txt\n",
      "Saved article: output_texts\\Netclan20241034.txt\n",
      "Saved article: output_texts\\Netclan20241035.txt\n",
      "Saved article: output_texts\\Netclan20241036.txt\n",
      "Saved article: output_texts\\Netclan20241037.txt\n",
      "Saved article: output_texts\\Netclan20241038.txt\n",
      "Saved article: output_texts\\Netclan20241039.txt\n",
      "Saved article: output_texts\\Netclan20241040.txt\n",
      "Saved article: output_texts\\Netclan20241041.txt\n",
      "Saved article: output_texts\\Netclan20241042.txt\n",
      "Saved article: output_texts\\Netclan20241043.txt\n",
      "Saved article: output_texts\\Netclan20241044.txt\n",
      "Saved article: output_texts\\Netclan20241045.txt\n",
      "Saved article: output_texts\\Netclan20241046.txt\n",
      "Saved article: output_texts\\Netclan20241047.txt\n",
      "Saved article: output_texts\\Netclan20241048.txt\n",
      "Saved article: output_texts\\Netclan20241049.txt\n",
      "Saved article: output_texts\\Netclan20241050.txt\n",
      "Saved article: output_texts\\Netclan20241051.txt\n",
      "Saved article: output_texts\\Netclan20241052.txt\n",
      "Saved article: output_texts\\Netclan20241053.txt\n",
      "Saved article: output_texts\\Netclan20241054.txt\n",
      "Saved article: output_texts\\Netclan20241055.txt\n",
      "Saved article: output_texts\\Netclan20241056.txt\n",
      "Saved article: output_texts\\Netclan20241057.txt\n",
      "Saved article: output_texts\\Netclan20241058.txt\n",
      "Saved article: output_texts\\Netclan20241059.txt\n",
      "Saved article: output_texts\\Netclan20241060.txt\n",
      "Saved article: output_texts\\Netclan20241061.txt\n",
      "Saved article: output_texts\\Netclan20241062.txt\n",
      "Saved article: output_texts\\Netclan20241063.txt\n",
      "Saved article: output_texts\\Netclan20241064.txt\n",
      "Saved article: output_texts\\Netclan20241065.txt\n",
      "Saved article: output_texts\\Netclan20241066.txt\n",
      "Saved article: output_texts\\Netclan20241067.txt\n",
      "Saved article: output_texts\\Netclan20241068.txt\n",
      "Saved article: output_texts\\Netclan20241069.txt\n",
      "Saved article: output_texts\\Netclan20241070.txt\n",
      "Saved article: output_texts\\Netclan20241071.txt\n",
      "Saved article: output_texts\\Netclan20241072.txt\n",
      "Saved article: output_texts\\Netclan20241073.txt\n",
      "Saved article: output_texts\\Netclan20241074.txt\n",
      "Saved article: output_texts\\Netclan20241075.txt\n",
      "Saved article: output_texts\\Netclan20241076.txt\n",
      "Saved article: output_texts\\Netclan20241077.txt\n",
      "Saved article: output_texts\\Netclan20241078.txt\n",
      "Saved article: output_texts\\Netclan20241079.txt\n",
      "Saved article: output_texts\\Netclan20241080.txt\n",
      "Saved article: output_texts\\Netclan20241081.txt\n",
      "Saved article: output_texts\\Netclan20241082.txt\n",
      "Saved article: output_texts\\Netclan20241083.txt\n",
      "Saved article: output_texts\\Netclan20241084.txt\n",
      "Saved article: output_texts\\Netclan20241085.txt\n",
      "Saved article: output_texts\\Netclan20241086.txt\n",
      "Saved article: output_texts\\Netclan20241087.txt\n",
      "Saved article: output_texts\\Netclan20241088.txt\n",
      "Saved article: output_texts\\Netclan20241089.txt\n",
      "Saved article: output_texts\\Netclan20241090.txt\n",
      "Saved article: output_texts\\Netclan20241091.txt\n",
      "Saved article: output_texts\\Netclan20241092.txt\n",
      "Saved article: output_texts\\Netclan20241093.txt\n",
      "Saved article: output_texts\\Netclan20241094.txt\n",
      "Saved article: output_texts\\Netclan20241095.txt\n",
      "Saved article: output_texts\\Netclan20241096.txt\n",
      "Saved article: output_texts\\Netclan20241097.txt\n",
      "Saved article: output_texts\\Netclan20241098.txt\n",
      "Saved article: output_texts\\Netclan20241099.txt\n",
      "Saved article: output_texts\\Netclan20241100.txt\n",
      "Saved article: output_texts\\Netclan20241101.txt\n",
      "Saved article: output_texts\\Netclan20241102.txt\n",
      "Saved article: output_texts\\Netclan20241103.txt\n",
      "Saved article: output_texts\\Netclan20241104.txt\n",
      "Saved article: output_texts\\Netclan20241105.txt\n",
      "Saved article: output_texts\\Netclan20241106.txt\n",
      "Saved article: output_texts\\Netclan20241107.txt\n",
      "Saved article: output_texts\\Netclan20241108.txt\n",
      "Saved article: output_texts\\Netclan20241109.txt\n",
      "Saved article: output_texts\\Netclan20241110.txt\n",
      "Saved article: output_texts\\Netclan20241111.txt\n",
      "Saved article: output_texts\\Netclan20241112.txt\n",
      "Saved article: output_texts\\Netclan20241113.txt\n",
      "Saved article: output_texts\\Netclan20241114.txt\n",
      "Saved article: output_texts\\Netclan20241115.txt\n",
      "Saved article: output_texts\\Netclan20241116.txt\n",
      "Saved article: output_texts\\Netclan20241117.txt\n",
      "Saved article: output_texts\\Netclan20241118.txt\n",
      "Saved article: output_texts\\Netclan20241119.txt\n",
      "Saved article: output_texts\\Netclan20241120.txt\n",
      "Saved article: output_texts\\Netclan20241121.txt\n",
      "Saved article: output_texts\\Netclan20241122.txt\n",
      "Saved article: output_texts\\Netclan20241123.txt\n",
      "Saved article: output_texts\\Netclan20241124.txt\n",
      "Saved article: output_texts\\Netclan20241125.txt\n",
      "Saved article: output_texts\\Netclan20241126.txt\n",
      "Saved article: output_texts\\Netclan20241127.txt\n",
      "Saved article: output_texts\\Netclan20241128.txt\n",
      "Saved article: output_texts\\Netclan20241129.txt\n",
      "Saved article: output_texts\\Netclan20241130.txt\n",
      "Saved article: output_texts\\Netclan20241131.txt\n",
      "Saved article: output_texts\\Netclan20241132.txt\n",
      "Saved article: output_texts\\Netclan20241133.txt\n",
      "Saved article: output_texts\\Netclan20241134.txt\n",
      "Saved article: output_texts\\Netclan20241135.txt\n",
      "Saved article: output_texts\\Netclan20241136.txt\n",
      "Saved article: output_texts\\Netclan20241137.txt\n",
      "Saved article: output_texts\\Netclan20241138.txt\n",
      "Saved article: output_texts\\Netclan20241139.txt\n",
      "Saved article: output_texts\\Netclan20241140.txt\n",
      "Saved article: output_texts\\Netclan20241141.txt\n",
      "Saved article: output_texts\\Netclan20241142.txt\n",
      "Saved article: output_texts\\Netclan20241143.txt\n",
      "Saved article: output_texts\\Netclan20241144.txt\n",
      "Saved article: output_texts\\Netclan20241145.txt\n",
      "Saved article: output_texts\\Netclan20241146.txt\n",
      "Saved article: output_texts\\Netclan20241147.txt\n",
      "Saved article: output_texts\\Netclan20241148.txt\n",
      "Saved article: output_texts\\Netclan20241149.txt\n",
      "Saved article: output_texts\\Netclan20241150.txt\n",
      "Saved article: output_texts\\Netclan20241151.txt\n",
      "Saved article: output_texts\\Netclan20241152.txt\n",
      "Saved article: output_texts\\Netclan20241153.txt\n",
      "Saved article: output_texts\\Netclan20241154.txt\n",
      "Saved article: output_texts\\Netclan20241155.txt\n",
      "Saved article: output_texts\\Netclan20241156.txt\n",
      "Saved article: output_texts\\Netclan20241157.txt\n",
      "Saved article: output_texts\\Netclan20241158.txt\n",
      "Saved article: output_texts\\Netclan20241159.txt\n",
      "Saved article: output_texts\\Netclan20241160.txt\n",
      "Saved article: output_texts\\Netclan20241161.txt\n",
      "Saved article: output_texts\\Netclan20241162.txt\n",
      "Saved article: output_texts\\Netclan20241163.txt\n",
      "Article extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input Excel file with full path\n",
    "excel_file = r\"C:\\Users\\admin\\Downloads\\Input.xlsx\"  # Use raw string (r\"...\") to avoid path issues\n",
    "output_dir = \"output_texts\"\n",
    "\n",
    "# Check if the file exists before proceeding\n",
    "if not os.path.exists(excel_file):\n",
    "    print(f\"Error: The file '{excel_file}' was not found. Please check the file path.\")\n",
    "else:\n",
    "    try:\n",
    "        # Ensure required library is installed for .xlsx files\n",
    "        df = pd.read_excel(excel_file, engine=\"openpyxl\")  # Use 'openpyxl' for .xlsx files\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Define a dummy extract_article_text function if it's not provided\n",
    "        def extract_article_text(url):\n",
    "            # This function should extract the title and text from the URL\n",
    "            # Replace this with actual web scraping logic\n",
    "            return \"Sample Title\", \"Sample article content.\"\n",
    "\n",
    "        # Iterate through the rows in the Excel file\n",
    "        for index, row in df.iterrows():\n",
    "            url_id = row.get(\"URL_ID\")  # Use .get() to avoid KeyError\n",
    "            url = row.get(\"URL\")\n",
    "            \n",
    "            if not url_id or not url:\n",
    "                print(f\"Skipping row {index} due to missing URL_ID or URL\")\n",
    "                continue\n",
    "            \n",
    "            # Extract article title and text\n",
    "            title, article_text = extract_article_text(url)\n",
    "            \n",
    "            if title and article_text:\n",
    "                output_file = os.path.join(output_dir, f\"{url_id}.txt\")\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(title + \"\\n\\n\" + article_text)\n",
    "                print(f\"Saved article: {output_file}\")\n",
    "            else:\n",
    "                print(f\"Skipping {url_id}: No content extracted.\")\n",
    "\n",
    "        print(\"Article extraction complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd888a-75bc-42d3-ac6e-a9196cc347c9",
   "metadata": {},
   "source": [
    "# # Article Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "070699ad-f8b1-4851-aab6-4a0e725fe582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: output_texts\\Netclan20241017.txt\n",
      "Saved article: output_texts\\Netclan20241018.txt\n",
      "Saved article: output_texts\\Netclan20241019.txt\n",
      "Saved article: output_texts\\Netclan20241020.txt\n",
      "Saved article: output_texts\\Netclan20241021.txt\n",
      "Saved article: output_texts\\Netclan20241022.txt\n",
      "Saved article: output_texts\\Netclan20241023.txt\n",
      "Saved article: output_texts\\Netclan20241024.txt\n",
      "Saved article: output_texts\\Netclan20241025.txt\n",
      "Saved article: output_texts\\Netclan20241026.txt\n",
      "Saved article: output_texts\\Netclan20241027.txt\n",
      "Saved article: output_texts\\Netclan20241028.txt\n",
      "Saved article: output_texts\\Netclan20241029.txt\n",
      "Saved article: output_texts\\Netclan20241030.txt\n",
      "Saved article: output_texts\\Netclan20241031.txt\n",
      "Saved article: output_texts\\Netclan20241032.txt\n",
      "Saved article: output_texts\\Netclan20241033.txt\n",
      "Saved article: output_texts\\Netclan20241034.txt\n",
      "Saved article: output_texts\\Netclan20241035.txt\n",
      "Saved article: output_texts\\Netclan20241036.txt\n",
      "Saved article: output_texts\\Netclan20241037.txt\n",
      "Saved article: output_texts\\Netclan20241038.txt\n",
      "Saved article: output_texts\\Netclan20241039.txt\n",
      "Saved article: output_texts\\Netclan20241040.txt\n",
      "Saved article: output_texts\\Netclan20241041.txt\n",
      "Saved article: output_texts\\Netclan20241042.txt\n",
      "Saved article: output_texts\\Netclan20241043.txt\n",
      "Saved article: output_texts\\Netclan20241044.txt\n",
      "Saved article: output_texts\\Netclan20241045.txt\n",
      "Saved article: output_texts\\Netclan20241046.txt\n",
      "Saved article: output_texts\\Netclan20241047.txt\n",
      "Saved article: output_texts\\Netclan20241048.txt\n",
      "Saved article: output_texts\\Netclan20241049.txt\n",
      "Saved article: output_texts\\Netclan20241050.txt\n",
      "Saved article: output_texts\\Netclan20241051.txt\n",
      "Saved article: output_texts\\Netclan20241052.txt\n",
      "Saved article: output_texts\\Netclan20241053.txt\n",
      "Saved article: output_texts\\Netclan20241054.txt\n",
      "Saved article: output_texts\\Netclan20241055.txt\n",
      "Saved article: output_texts\\Netclan20241056.txt\n",
      "Saved article: output_texts\\Netclan20241057.txt\n",
      "Saved article: output_texts\\Netclan20241058.txt\n",
      "Saved article: output_texts\\Netclan20241059.txt\n",
      "Saved article: output_texts\\Netclan20241060.txt\n",
      "Saved article: output_texts\\Netclan20241061.txt\n",
      "Saved article: output_texts\\Netclan20241062.txt\n",
      "Saved article: output_texts\\Netclan20241063.txt\n",
      "Saved article: output_texts\\Netclan20241064.txt\n",
      "Saved article: output_texts\\Netclan20241065.txt\n",
      "Saved article: output_texts\\Netclan20241066.txt\n",
      "Saved article: output_texts\\Netclan20241067.txt\n",
      "Saved article: output_texts\\Netclan20241068.txt\n",
      "Saved article: output_texts\\Netclan20241069.txt\n",
      "Saved article: output_texts\\Netclan20241070.txt\n",
      "Saved article: output_texts\\Netclan20241071.txt\n",
      "Saved article: output_texts\\Netclan20241072.txt\n",
      "Saved article: output_texts\\Netclan20241073.txt\n",
      "Saved article: output_texts\\Netclan20241074.txt\n",
      "Saved article: output_texts\\Netclan20241075.txt\n",
      "Saved article: output_texts\\Netclan20241076.txt\n",
      "Saved article: output_texts\\Netclan20241077.txt\n",
      "Saved article: output_texts\\Netclan20241078.txt\n",
      "Saved article: output_texts\\Netclan20241079.txt\n",
      "Saved article: output_texts\\Netclan20241080.txt\n",
      "Saved article: output_texts\\Netclan20241081.txt\n",
      "Saved article: output_texts\\Netclan20241082.txt\n",
      "Saved article: output_texts\\Netclan20241083.txt\n",
      "Saved article: output_texts\\Netclan20241084.txt\n",
      "Saved article: output_texts\\Netclan20241085.txt\n",
      "Saved article: output_texts\\Netclan20241086.txt\n",
      "Saved article: output_texts\\Netclan20241087.txt\n",
      "Saved article: output_texts\\Netclan20241088.txt\n",
      "Saved article: output_texts\\Netclan20241089.txt\n",
      "Saved article: output_texts\\Netclan20241090.txt\n",
      "Saved article: output_texts\\Netclan20241091.txt\n",
      "Saved article: output_texts\\Netclan20241092.txt\n",
      "Saved article: output_texts\\Netclan20241093.txt\n",
      "Saved article: output_texts\\Netclan20241094.txt\n",
      "Saved article: output_texts\\Netclan20241095.txt\n",
      "Saved article: output_texts\\Netclan20241096.txt\n",
      "Saved article: output_texts\\Netclan20241097.txt\n",
      "Saved article: output_texts\\Netclan20241098.txt\n",
      "Saved article: output_texts\\Netclan20241099.txt\n",
      "Saved article: output_texts\\Netclan20241100.txt\n",
      "Saved article: output_texts\\Netclan20241101.txt\n",
      "Saved article: output_texts\\Netclan20241102.txt\n",
      "Saved article: output_texts\\Netclan20241103.txt\n",
      "Saved article: output_texts\\Netclan20241104.txt\n",
      "Saved article: output_texts\\Netclan20241105.txt\n",
      "Saved article: output_texts\\Netclan20241106.txt\n",
      "Saved article: output_texts\\Netclan20241107.txt\n",
      "Saved article: output_texts\\Netclan20241108.txt\n",
      "Saved article: output_texts\\Netclan20241109.txt\n",
      "Saved article: output_texts\\Netclan20241110.txt\n",
      "Saved article: output_texts\\Netclan20241111.txt\n",
      "Saved article: output_texts\\Netclan20241112.txt\n",
      "Saved article: output_texts\\Netclan20241113.txt\n",
      "Saved article: output_texts\\Netclan20241114.txt\n",
      "Saved article: output_texts\\Netclan20241115.txt\n",
      "Saved article: output_texts\\Netclan20241116.txt\n",
      "Saved article: output_texts\\Netclan20241117.txt\n",
      "Saved article: output_texts\\Netclan20241118.txt\n",
      "Saved article: output_texts\\Netclan20241119.txt\n",
      "Saved article: output_texts\\Netclan20241120.txt\n",
      "Saved article: output_texts\\Netclan20241121.txt\n",
      "Saved article: output_texts\\Netclan20241122.txt\n",
      "Saved article: output_texts\\Netclan20241123.txt\n",
      "Saved article: output_texts\\Netclan20241124.txt\n",
      "Saved article: output_texts\\Netclan20241125.txt\n",
      "Saved article: output_texts\\Netclan20241126.txt\n",
      "Saved article: output_texts\\Netclan20241127.txt\n",
      "Saved article: output_texts\\Netclan20241128.txt\n",
      "Saved article: output_texts\\Netclan20241129.txt\n",
      "Saved article: output_texts\\Netclan20241130.txt\n",
      "Saved article: output_texts\\Netclan20241131.txt\n",
      "Saved article: output_texts\\Netclan20241132.txt\n",
      "Saved article: output_texts\\Netclan20241133.txt\n",
      "Saved article: output_texts\\Netclan20241134.txt\n",
      "Saved article: output_texts\\Netclan20241135.txt\n",
      "Saved article: output_texts\\Netclan20241136.txt\n",
      "Saved article: output_texts\\Netclan20241137.txt\n",
      "Saved article: output_texts\\Netclan20241138.txt\n",
      "Saved article: output_texts\\Netclan20241139.txt\n",
      "Saved article: output_texts\\Netclan20241140.txt\n",
      "Saved article: output_texts\\Netclan20241141.txt\n",
      "Saved article: output_texts\\Netclan20241142.txt\n",
      "Saved article: output_texts\\Netclan20241143.txt\n",
      "Saved article: output_texts\\Netclan20241144.txt\n",
      "Saved article: output_texts\\Netclan20241145.txt\n",
      "Saved article: output_texts\\Netclan20241146.txt\n",
      "Saved article: output_texts\\Netclan20241147.txt\n",
      "Saved article: output_texts\\Netclan20241148.txt\n",
      "Saved article: output_texts\\Netclan20241149.txt\n",
      "Saved article: output_texts\\Netclan20241150.txt\n",
      "Saved article: output_texts\\Netclan20241151.txt\n",
      "Saved article: output_texts\\Netclan20241152.txt\n",
      "Saved article: output_texts\\Netclan20241153.txt\n",
      "Saved article: output_texts\\Netclan20241154.txt\n",
      "Saved article: output_texts\\Netclan20241155.txt\n",
      "Saved article: output_texts\\Netclan20241156.txt\n",
      "Saved article: output_texts\\Netclan20241157.txt\n",
      "Saved article: output_texts\\Netclan20241158.txt\n",
      "Saved article: output_texts\\Netclan20241159.txt\n",
      "Saved article: output_texts\\Netclan20241160.txt\n",
      "Saved article: output_texts\\Netclan20241161.txt\n",
      "Saved article: output_texts\\Netclan20241162.txt\n",
      "Saved article: output_texts\\Netclan20241163.txt\n",
      "Article extraction and analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# Define text analysis function before using it\n",
    "def analyze_text(text):\n",
    "    \"\"\"Analyzes text and returns word count & character count.\"\"\"\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    return [word_count, char_count]\n",
    "\n",
    "# Initialize output data storage\n",
    "output_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row.get(\"URL_ID\")\n",
    "    url = row.get(\"URL\")\n",
    "\n",
    "    if not url_id or not url:\n",
    "        print(f\"Skipping row {index} due to missing URL_ID or URL\")\n",
    "        continue\n",
    "\n",
    "    # Extract article title and text\n",
    "    title, article_text = extract_article_text(url)\n",
    "\n",
    "    if title and article_text:\n",
    "        # Perform analysis\n",
    "        analysis_results = analyze_text(article_text)  # ✅ Now this function is defined\n",
    "        output_data.append([url_id, url] + analysis_results)\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{url_id}.txt\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(title + \"\\n\\n\" + article_text)\n",
    "        print(f\"Saved article: {output_file}\")\n",
    "\n",
    "print(\"Article extraction and analysis complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0199fb6-8ccc-4346-88a3-2c060c943149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column order from \"Output Data Structure.xlsx\"\n",
    "columns = [\"URL_ID\", \"URL\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\",\n",
    "           \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "           \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
    "           \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dfa7c63-8274-4817-b60d-90ae74e18a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(output_data[0]))  # Print the length of first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63bd8463-699a-46bc-9783-96efef991351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(columns))  # Print the length of columns list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae49fb5-c159-456e-b35b-2733ddf4e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis results saved successfully!\n",
      "Data extraction and analysis complete! Output saved in 'Output Data Structure.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Define column names based on output_data structure\n",
    "columns = [\"URL_ID\", \"URL\", \"Word_Count\", \"Character_Count\"]  # Adjust based on analyze_text()\n",
    "\n",
    "# Save analysis results to Excel\n",
    "output_df = pd.DataFrame(output_data, columns=columns)  # Ensure columns match data structure\n",
    "output_df.to_excel(\"Output Data Structure.xlsx\", index=False)\n",
    "\n",
    "print(\"Analysis results saved successfully!\")\n",
    "print(\"Data extraction and analysis complete! Output saved in 'Output Data Structure.xlsx'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016bed36-7cf2-43cf-8bf3-dec1777fd0be",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48198e15-e0e7-4c7b-a99d-202f1ffa5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c157a97-d84e-4dba-88bf-2cc52109a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\admin/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data download completed.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"NLTK data download completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab2f527c-2332-43b5-87d0-bcc1c6f218fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate text metrics\n",
    "def analyze_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_filtered = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae95d626-b19e-4afb-a3ad-35331ccbafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e03c64a-978c-408c-b512-f245a615db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability Metrics\n",
    "def calculate_readability(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words_filtered = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    complex_word_count = sum(1 for word in words_filtered if len(word) > 6)\n",
    "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "    percentage_complex_words = complex_word_count / len(words_filtered) if words_filtered else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    return {\n",
    "        \"complex_word_count\": complex_word_count,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"percentage_complex_words\": percentage_complex_words,\n",
    "        \"fog_index\": fog_index\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a4497fd-6bb6-469f-af15-6e0c7441b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Statistics\n",
    "def calculate_word_statistics(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words_filtered = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    avg_words_per_sentence = len(words_filtered) / len(sentences) if sentences else 0\n",
    "    word_count = len(words_filtered)\n",
    "    syllables_per_word = np.mean([sum(1 for ch in word.lower() if ch in 'aeiou') for word in words_filtered]) if words_filtered else 0\n",
    "    \n",
    "    return {\n",
    "        \"avg_words_per_sentence\": avg_words_per_sentence,\n",
    "        \"word_count\": word_count,\n",
    "        \"syllables_per_word\": syllables_per_word\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37ae5bfb-460b-4948-963e-d150a7152f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal Pronouns Count\n",
    "def count_personal_pronouns(text):\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    return personal_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0df67b4c-574c-492d-9a68-a7d55feded12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_length(words_filtered):\n",
    "    avg_word_length = sum(len(word) for word in words_filtered) / len(words_filtered)\n",
    "    return avg_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7b98419-ea01-4eb0-8626-c0127bb87eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function():\n",
    "    return [\n",
    "        polarity, \n",
    "        subjectivity, \n",
    "        avg_sentence_length, \n",
    "        percentage_complex_words, \n",
    "        fog_index, \n",
    "        avg_words_per_sentence,\n",
    "        complex_word_count, \n",
    "        word_count, \n",
    "        syllables_per_word, \n",
    "        personal_pronouns, \n",
    "        avg_word_length\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cb1f330-e4ab-4dd7-9411-700967850f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for processed data\n",
    "processed_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce67cd08-89be-458a-add7-63175ce85319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'example.com/1', 0.1, 0.2, 15, 30, 10, 12, 5, 100, 1.5, 3, 4.2], [2, 'example.com/2', 0.1, 0.2, 15, 30, 10, 12, 5, 100, 1.5, 3, 4.2]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure df_structure is defined (example)\n",
    "df_structure = pd.DataFrame({'URL_ID': [1, 2], 'URL': ['example.com/1', 'example.com/2'], 'Text': [\"Sample text 1\", \"Sample text 2\"]})\n",
    "\n",
    "# Initialize an empty list to store processed data\n",
    "processed_data = []\n",
    "\n",
    "# Define analyze_text function (example placeholder)\n",
    "def analyze_text(text):\n",
    "    return [0.1, 0.2, 15, 30, 10, 12, 5, 100, 1.5, 3, 4.2]  # Replace with actual analysis logic\n",
    "\n",
    "# Iterate over each row to analyze text\n",
    "for index, row in df_structure.iterrows():\n",
    "    text = row['Text']  # Replace with actual text extraction logic\n",
    "    analysis_results = analyze_text(text)\n",
    "    processed_data.append([row['URL_ID'], row['URL']] + analysis_results)\n",
    "\n",
    "# Print result\n",
    "print(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca21703d-e79e-45ed-bfc0-9dcdd2e800f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with results\n",
    "columns = ['URL_ID', 'URL', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
    "           'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "           'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS',\n",
    "           'AVG WORD LENGTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e8cf579-39b1-4259-88cb-fcc36076ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Output saved as Analyzed_Text_Output.xlsx\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.DataFrame(processed_data, columns=columns)\n",
    "\n",
    "# Save the output\n",
    "output_file = \"Analyzed_Text_Output.xlsx\"\n",
    "df_output.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Analysis completed. Output saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e997a3-3059-4b50-83ca-7dd74cfcde43",
   "metadata": {},
   "source": [
    "# Text Analysis Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76462db0-d0de-4917-a61c-8706417d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2febd06f-19fc-4561-bf07-1ee9120bd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09b014f3-c4b5-44c0-8ce7-4f02e5fcf560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'demonstrate', 'stopword', 'removal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
    "\n",
    "# Example text\n",
    "text = \"This is an example sentence to demonstrate stopword removal in Python.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()  # You can also use nltk.word_tokenize(text) if needed\n",
    "\n",
    "# Load Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords and keep only alphanumeric words\n",
    "words_cleaned = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "print(words_cleaned)  # Output: ['example', 'sentence', 'demonstrate', 'stopword', 'removal', 'Python']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78288877-0f84-4d8b-bebb-38d147a7910c",
   "metadata": {},
   "source": [
    "# Positive words loaded Negative words loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5668ecf6-5dab-4487-adff-b5ab1f146490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words loaded: 2006\n",
      "Negative words loaded: 4783\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the full path to the files\n",
    "downloads_path = r\"C:\\Users\\admin\\Downloads\"\n",
    "positive_file = os.path.join(downloads_path, \"positive-words.txt\")\n",
    "negative_file = os.path.join(downloads_path, \"negative-words.txt\")\n",
    "\n",
    "# Load Positive and Negative Words\n",
    "with open(positive_file, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\") as file:\n",
    "    positive_words = {line.strip() for line in file}\n",
    "\n",
    "with open(negative_file, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\") as file:\n",
    "    negative_words = {line.strip() for line in file}\n",
    "\n",
    "print(\"Positive words loaded:\", len(positive_words))\n",
    "print(\"Negative words loaded:\", len(negative_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80e35ea6-c401-424a-887d-c75bf091b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Positive and Negative Score\n",
    "positive_score = sum(1 for word in words_cleaned if word in positive_words)\n",
    "negative_score = sum(1 for word in words_cleaned if word in negative_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d5ff26d1-3cde-49d3-9684-15501da1ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Polarity Score\n",
    "polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dc5defb8-6070-4f89-8210-d8b957df5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Subjectivity Score\n",
    "subjectivity_score = (positive_score + negative_score) / (len(words_cleaned) + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5d1dab7d-5556-4d5b-9dc7-7e9a31090fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download sentence tokenizer\n",
    "nltk.download('stopwords')  # Download stopwords dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5117460c-4582-4e1e-8337-c416f018fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fog Index: 1.49\n",
      "Average Sentence Length: 3.33\n",
      "Percentage of Complex Words: 40.00%\n",
      "Complex Word Count: 4\n",
      "Total Word Count: 10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text (Replace with your actual text)\n",
    "text = \"This is an example sentence. It contains multiple words. Some of them are complex.\"\n",
    "\n",
    "# Tokenize sentences (split using punctuation)\n",
    "sentences = re.split(r'[.!?]', text)  # Split text into sentences using common sentence-ending punctuation\n",
    "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
    "\n",
    "# Tokenize words (split using whitespace)\n",
    "words = re.findall(r'\\b\\w+\\b', text)  # Extract words using regex\n",
    "\n",
    "# Define a basic stopword list (replace with a more comprehensive list if needed)\n",
    "stop_words = {\"is\", \"an\", \"the\", \"it\", \"of\", \"to\", \"a\", \"and\", \"in\", \"on\", \"for\", \"with\", \"at\", \"by\", \"from\"}\n",
    "\n",
    "# Remove stopwords and keep only alphanumeric words\n",
    "words_cleaned = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Ensure sentences and words_cleaned are not empty to avoid division errors\n",
    "avg_sentence_length = len(words_cleaned) / len(sentences) if sentences else 0\n",
    "\n",
    "# Count complex words (words with more than 2 vowels)\n",
    "complex_word_count = sum(1 for word in words_cleaned if sum(1 for ch in word if ch.lower() in \"aeiou\") > 2)\n",
    "\n",
    "# Avoid division by zero\n",
    "percentage_complex_words = complex_word_count / len(words_cleaned) if words_cleaned else 0\n",
    "\n",
    "# Calculate Fog Index\n",
    "fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Average words per sentence (same as avg_sentence_length)\n",
    "avg_words_per_sentence = avg_sentence_length  \n",
    "\n",
    "# Total word count\n",
    "word_count = len(words_cleaned)\n",
    "\n",
    "# Print results\n",
    "print(f\"Fog Index: {fog_index:.2f}\")\n",
    "print(f\"Average Sentence Length: {avg_sentence_length:.2f}\")\n",
    "print(f\"Percentage of Complex Words: {percentage_complex_words:.2%}\")\n",
    "print(f\"Complex Word Count: {complex_word_count}\")\n",
    "print(f\"Total Word Count: {word_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2353d57d-3afb-4395-b7a5-d7df143ba3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Syllables Per Word\n",
    "syllables_per_word = (\n",
    "    sum(sum(1 for ch in word if ch.lower() in \"aeiou\") for word in words_cleaned) / len(words_cleaned) \n",
    "    if words_cleaned else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "315460d3-9360-497e-88c8-e496a803730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Average Word Length\n",
    "avg_word_length = sum(len(word) for word in words_cleaned) / len(words_cleaned) if words_cleaned else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca5f7b-00cb-4ca2-b603-1330da62f2ab",
   "metadata": {},
   "source": [
    "# calculate_readability_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8f76cec6-88ad-4747-8e1c-7e7e97abedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability_metrics():\n",
    "    return {\n",
    "        \"POSITIVE SCORE\": positive_score,\n",
    "        \"NEGATIVE SCORE\": negative_score,\n",
    "        \"POLARITY SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "        \"WORD COUNT\": word_count,\n",
    "        \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "        \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_length\n",
    "    }\n",
    "\n",
    "# Example usage (Ensure variables are defined before calling the function)\n",
    "# readability_scores = calculate_readability_metrics()\n",
    "# print(readability_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "72b5d74f-a435-4f7c-a4e7-8b53c19feb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text Data\n",
    "text = \"\"\"Your text data here.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc06856-98e2-4fe6-a624-79f5f8c4a1c5",
   "metadata": {},
   "source": [
    "# # Text Analysis Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aa517450-338b-4670-8e84-9b6842f84b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def calculate_metrics(text):\n",
    "    # Tokenize words using regex\n",
    "    words_cleaned = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    # Tokenize sentences using regex (split on punctuation)\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty sentences\n",
    "\n",
    "    # Count complex words (words with more than 2 vowels)\n",
    "    def count_vowels(word):\n",
    "        return sum(1 for ch in word if ch.lower() in \"aeiou\")\n",
    "\n",
    "    complex_word_count = sum(1 for word in words_cleaned if count_vowels(word) > 2)\n",
    "\n",
    "    # Compute metrics (keeping the original logic)\n",
    "    avg_sentence_length = len(words_cleaned) / len(sentences) if sentences else 0\n",
    "    percentage_complex_words = complex_word_count / len(words_cleaned) if words_cleaned else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = avg_sentence_length\n",
    "    word_count = len(words_cleaned)\n",
    "    avg_word_length = sum(len(word) for word in words_cleaned) / len(words_cleaned) if words_cleaned else 0\n",
    "\n",
    "    return {\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "        \"WORD COUNT\": word_count,\n",
    "        \"AVG WORD LENGTH\": avg_word_length\n",
    "    }\n",
    "\n",
    "# Calling the function (Ensure 'text' is defined before calling)\n",
    "results = calculate_metrics(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "40f0644d-0bc4-4daf-8311-500620fe09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b6102ec6-2bd0-4d53-b113-7208dc72c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Excel\n",
    "df_results.to_excel(\"Output Data Structure.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fe7480fa-8880-410d-b45f-950e63ac08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  4.0                          0.0        1.6   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                               4.0                   0           4   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              4.0  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f8805-4f6c-4a2d-a647-5714e35bb417",
   "metadata": {},
   "source": [
    "# Output Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3c4870ed-a85d-42fc-84c3-61acc89e5b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of columns: 15\n",
      "Actual number of columns in output data: 14\n",
      "Column mismatch detected. Expected 15 columns, but found 14 columns.\n",
      "Data processed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load input data\n",
    "input_file = r\"C:\\Users\\admin\\Downloads\\Input.xlsx\"\n",
    "data = pd.read_excel(input_file)\n",
    "\n",
    "# Load positive and negative words\n",
    "positive_words = set(open(r\"C:\\Users\\admin\\Downloads\\positive-words.txt\").read().split())\n",
    "negative_words = set(open(r\"C:\\Users\\admin\\Downloads\\negative-words.txt\").read().split())\n",
    "\n",
    "# Stopwords list (a custom list or a common list can be used)\n",
    "stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \n",
    "    \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \n",
    "    \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \n",
    "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \n",
    "    \"don\", \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \n",
    "    \"hadn\", \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \n",
    "    \"wouldn\"\n",
    "])\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all non-alphabetical characters (punctuation, numbers, etc.)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize by splitting into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def analyze_text(text):\n",
    "    words = clean_text(text)\n",
    "    sentences = text.split('.')  # Simple sentence split by period\n",
    "    \n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    \n",
    "    avg_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "    complex_words = [word for word in words if len(re.findall(r'[aeiou]', word)) > 2]\n",
    "    percentage_complex_words = len(complex_words) / len(words) if len(words) > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "    \n",
    "    word_count = len(words)\n",
    "    syllable_per_word = sum(len(re.findall(r'[aeiou]', word)) for word in words) / word_count if word_count > 0 else 0\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Return only 13 values, so it matches the 14-column structure\n",
    "    return [positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
    "            percentage_complex_words, fog_index, avg_words_per_sentence, len(complex_words), word_count,\n",
    "            syllable_per_word, personal_pronouns, avg_word_length]\n",
    "\n",
    "# Process all input text data\n",
    "output_data = []\n",
    "for index, row in data.iterrows():\n",
    "    text = row['URL']  # Use 'URL' as the column containing the text\n",
    "    output_data.append([row['URL_ID']] + analyze_text(text))  # Ensure the first column is URL_ID\n",
    "\n",
    "# Load output structure\n",
    "output_structure = pd.read_excel(r\"C:\\Users\\admin\\Downloads\\Output Data Structure.xlsx\")\n",
    "columns = output_structure.columns.tolist()\n",
    "\n",
    "# Print column lengths to debug the mismatch\n",
    "print(f\"Expected number of columns: {len(columns)}\")\n",
    "print(f\"Actual number of columns in output data: {len(output_data[0])}\")\n",
    "\n",
    "# Ensure the number of columns in output data matches the structure (14 or 15 columns)\n",
    "if len(columns) != len(output_data[0]):\n",
    "    print(f\"Column mismatch detected. Expected {len(columns)} columns, but found {len(output_data[0])} columns.\")\n",
    "    # Adjust the number of columns by adding an extra dummy column if required\n",
    "    while len(columns) < len(output_data[0]):\n",
    "        columns.append('Extra Column')  # Add a placeholder column name\n",
    "    \n",
    "    while len(columns) > len(output_data[0]):\n",
    "        output_data[0].append(None)  # Add None to data to match the column count\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_df = pd.DataFrame(output_data, columns=columns)\n",
    "output_df.to_excel(\"Analyzed_Output.xlsx\", index=False)\n",
    "\n",
    "print(\"Data processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6aa7d8-81c9-4436-9556-058c5125f23a",
   "metadata": {},
   "source": [
    "# Row Data Preview: Print a preview of the text data being processed for each row, especially if you suspect data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e73fb769-264e-4316-8405-7f2b71134c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 146 (URL_ID: Netclan20241163): https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing row {index} (URL_ID: {row['URL_ID']}): {text[:100]}...\")  # Preview of first 100 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687e61e-d562-455e-a661-36f2e68911f8",
   "metadata": {},
   "source": [
    "# Debugging Cleaned Text: Print the cleaned text after removing stopwords and punctuation to verify that the cleaning process is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c1fd1dad-f6c4-4644-a5fe-6ec80367ef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 146 (URL_ID: Netclan20241163): https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing row {index} (URL_ID: {row['URL_ID']}): {text[:100]}...\")  # Preview of first 100 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84d83b-0cf0-4c06-901e-4f3a25ff8e7a",
   "metadata": {},
   "source": [
    "# Analyzed Scores: Print the individual scores (positive, negative, polarity, etc.) to track the analysis progress for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3cff436b-acee-4010-a116-d30249e56b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Positive Score: {positive_score}, Negative Score: {negative_score}, Polarity Score: {polarity_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb6021-08d3-487b-9b41-657390ac7e93",
   "metadata": {},
   "source": [
    "# Columns Mismatch Debugging: If the column mismatch is an issue, you can print the column names for both the output_data and the output_structure to get more insight into where the mismatch might be happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "824c27b7-1a4d-4125-b906-d5ee859de05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Data Columns: 15 columns\n",
      "Expected Columns: 15 columns\n",
      "Columns in Output Data: ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
      "Columns in Output Structure: ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output Data Columns: {len(output_data[0])} columns\")\n",
    "print(f\"Expected Columns: {len(columns)} columns\")\n",
    "print(f\"Columns in Output Data: {output_df.columns.tolist()}\")\n",
    "print(f\"Columns in Output Structure: {columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "415b0455-5a68-4ad8-a259-c2e879cd5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of output data:\n",
      "[['Netclan20241017', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 44.0, 0, 128.0, None], ['Netclan20241018', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 50.0, 0, 141.0], ['Netclan20241019', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 94.0], ['Netclan20241020', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 62.0, 0, 177.0], ['Netclan20241021', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 66.0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of output data:\")\n",
    "print(output_data[:5])  # Print first 5 rows of output data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869824d-a6ed-4de3-93ac-d2a60fa4e5e8",
   "metadata": {},
   "source": [
    "# Additional Debug Prints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8604acb3-d572-4281-bf64-75c35ce1e857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0 (URL_ID: Netclan20241017): https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-opt...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaiandmlbasedyoutubeanalyticsandcontentcreationtoolforoptimizingsubscriberengagementandcontentstrategy']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaiandmlbasedyoutubeanalyticsandcontentcreationtoolforoptimizingsubscriberengagementandcontentstrategy']...\n",
      "Output Row for URL_ID Netclan20241017: ['Netclan20241017', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 44.0, 0, 128.0]\n",
      "Processing row 1 (URL_ID: Netclan20241018): https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-ex...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomenhancingfrontendfeaturesandfunctionalityforimproveduserexperienceanddashboardaccuracyinpartnerhospitalapplication']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomenhancingfrontendfeaturesandfunctionalityforimproveduserexperienceanddashboardaccuracyinpartnerhospitalapplication']...\n",
      "Output Row for URL_ID Netclan20241018: ['Netclan20241018', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 50.0, 0, 141.0]\n",
      "Processing row 2 (URL_ID: Netclan20241019): https://insights.blackcoffer.com/roas-dashboard-for-campaign-wise-google-ads-budget-tracking-using-g...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomroasdashboardforcampaignwisegoogleadsbudgettrackingusinggoogleadsap']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomroasdashboardforcampaignwisegoogleadsbudgettrackingusinggoogleadsap']...\n",
      "Output Row for URL_ID Netclan20241019: ['Netclan20241019', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 94.0]\n",
      "Processing row 3 (URL_ID: Netclan20241020): https://insights.blackcoffer.com/efficient-processing-and-analysis-of-financial-data-from-pdf-files-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomefficientprocessingandanalysisoffinancialdatafrompdffilesaddressingformattinginconsistenciesandensuringdataintegrityforatoyotadealershipmanagementfirm']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomefficientprocessingandanalysisoffinancialdatafrompdffilesaddressingformattinginconsistenciesandensuringdataintegrityforatoyotadealershipmanagementfirm']...\n",
      "Output Row for URL_ID Netclan20241020: ['Netclan20241020', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 62.0, 0, 177.0]\n",
      "Processing row 4 (URL_ID: Netclan20241021): https://insights.blackcoffer.com/development-of-ea-robot-for-automated-trading/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdevelopmentofearobotforautomatedtrading']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdevelopmentofearobotforautomatedtrading']...\n",
      "Output Row for URL_ID Netclan20241021: ['Netclan20241021', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 66.0]\n",
      "Processing row 5 (URL_ID: Netclan20241022): https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-opt...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaiandmlbasedyoutubeanalyticsandcontentcreationtoolforoptimizingsubscriberengagementandcontentstrategy']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaiandmlbasedyoutubeanalyticsandcontentcreationtoolforoptimizingsubscriberengagementandcontentstrategy']...\n",
      "Output Row for URL_ID Netclan20241022: ['Netclan20241022', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 44.0, 0, 128.0]\n",
      "Processing row 6 (URL_ID: Netclan20241023): https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-ex...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomenhancingfrontendfeaturesandfunctionalityforimproveduserexperienceanddashboardaccuracyinpartnerhospitalapplication']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomenhancingfrontendfeaturesandfunctionalityforimproveduserexperienceanddashboardaccuracyinpartnerhospitalapplication']...\n",
      "Output Row for URL_ID Netclan20241023: ['Netclan20241023', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 50.0, 0, 141.0]\n",
      "Processing row 7 (URL_ID: Netclan20241024): https://insights.blackcoffer.com/roas-dashboard-for-campaign-wise-google-ads-budget-tracking-using-g...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomroasdashboardforcampaignwisegoogleadsbudgettrackingusinggoogleadsap']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomroasdashboardforcampaignwisegoogleadsbudgettrackingusinggoogleadsap']...\n",
      "Output Row for URL_ID Netclan20241024: ['Netclan20241024', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 94.0]\n",
      "Processing row 8 (URL_ID: Netclan20241025): https://insights.blackcoffer.com/efficient-processing-and-analysis-of-financial-data-from-pdf-files-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomefficientprocessingandanalysisoffinancialdatafrompdffilesaddressingformattinginconsistenciesandensuringdataintegrityforatoyotadealershipmanagementfirm']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomefficientprocessingandanalysisoffinancialdatafrompdffilesaddressingformattinginconsistenciesandensuringdataintegrityforatoyotadealershipmanagementfirm']...\n",
      "Output Row for URL_ID Netclan20241025: ['Netclan20241025', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 62.0, 0, 177.0]\n",
      "Processing row 9 (URL_ID: Netclan20241026): https://insights.blackcoffer.com/transforming-and-managing-a-large-scale-sql-pedigree-database-to-ne...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomtransformingandmanagingalargescalesqlpedigreedatabasetoneojgraphdb']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomtransformingandmanagingalargescalesqlpedigreedatabasetoneojgraphdb']...\n",
      "Output Row for URL_ID Netclan20241026: ['Netclan20241026', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 30.0, 0, 93.0]\n",
      "Processing row 10 (URL_ID: Netclan20241027): https://insights.blackcoffer.com/enhancing-model-accuracy-from-58-to-over-90-strategies-for-improvin...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomenhancingmodelaccuracyfromtooverstrategiesforimprovingpredictiveperformance']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomenhancingmodelaccuracyfromtooverstrategiesforimprovingpredictiveperformance']...\n",
      "Output Row for URL_ID Netclan20241027: ['Netclan20241027', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 34.0, 0, 102.0]\n",
      "Processing row 11 (URL_ID: Netclan20241028): https://insights.blackcoffer.com/securing-sensitive-financial-data-with-privacy-preserving-machine-l...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomsecuringsensitivefinancialdatawithprivacypreservingmachinelearningforpredictiveanalytics']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomsecuringsensitivefinancialdatawithprivacypreservingmachinelearningforpredictiveanalytics']...\n",
      "Output Row for URL_ID Netclan20241028: ['Netclan20241028', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 39.0, 0, 115.0]\n",
      "Processing row 12 (URL_ID: Netclan20241029): https://insights.blackcoffer.com/enhancing-data-collection-for-research-institutions-addressing-surv...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomenhancingdatacollectionforresearchinstitutionsaddressingsurveyfatigueandincorporatingverbalcommunicationforricherinsights']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomenhancingdatacollectionforresearchinstitutionsaddressingsurveyfatigueandincorporatingverbalcommunicationforricherinsights']...\n",
      "Output Row for URL_ID Netclan20241029: ['Netclan20241029', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 52.0, 0, 148.0]\n",
      "Processing row 13 (URL_ID: Netclan20241030): https://insights.blackcoffer.com/analyzing-the-impact-of-positive-emotions-and-pandemic-severity-on-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanalyzingtheimpactofpositiveemotionsandpandemicseverityonmentalhealthandresilienceamongentrepreneursinsightsandpredictivemodeling']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanalyzingtheimpactofpositiveemotionsandpandemicseverityonmentalhealthandresilienceamongentrepreneursinsightsandpredictivemodeling']...\n",
      "Output Row for URL_ID Netclan20241030: ['Netclan20241030', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 56.0, 0, 156.0]\n",
      "Processing row 14 (URL_ID: Netclan20241031): https://insights.blackcoffer.com/dynamic-brand-centric-dashboard-for-automotive-dealerships-pdf-to-f...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdynamicbrandcentricdashboardforautomotivedealershipspdftofinancialinsightswithflaskreactarchitectureandawscloudhosting']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdynamicbrandcentricdashboardforautomotivedealershipspdftofinancialinsightswithflaskreactarchitectureandawscloudhosting']...\n",
      "Output Row for URL_ID Netclan20241031: ['Netclan20241031', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 47.0, 0, 145.0]\n",
      "Processing row 15 (URL_ID: Netclan20241032): https://insights.blackcoffer.com/cloud-based-data-modeling-and-analysis-platform-with-drag-and-drop-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcloudbaseddatamodelingandanalysisplatformwithdraganddropinterfaceandopenaiapiintegrationforsimulationinsights']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcloudbaseddatamodelingandanalysisplatformwithdraganddropinterfaceandopenaiapiintegrationforsimulationinsights']...\n",
      "Output Row for URL_ID Netclan20241032: ['Netclan20241032', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 49.0, 0, 136.0]\n",
      "Processing row 16 (URL_ID: Netclan20241033): https://insights.blackcoffer.com/voter-profile-analysis-and-search-application-for-targeted-campaign...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomvoterprofileanalysisandsearchapplicationfortargetedcampaignengagementusinggovernmentvoterdata']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomvoterprofileanalysisandsearchapplicationfortargetedcampaignengagementusinggovernmentvoterdata']...\n",
      "Output Row for URL_ID Netclan20241033: ['Netclan20241033', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 42.0, 0, 120.0]\n",
      "Processing row 17 (URL_ID: Netclan20241034): https://insights.blackcoffer.com/bert-based-classification-of-individuals-and-organizations-into-two...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombertbasedclassificationofindividualsandorganizationsintotwocategoriesusingnaturallanguageprocessing']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombertbasedclassificationofindividualsandorganizationsintotwocategoriesusingnaturallanguageprocessing']...\n",
      "Output Row for URL_ID Netclan20241034: ['Netclan20241034', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 48.0, 0, 126.0]\n",
      "Processing row 18 (URL_ID: Netclan20241035): https://insights.blackcoffer.com/comprehensive-analysis-of-solana-and-ethereum-contributors-using-gi...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcomprehensiveanalysisofsolanaandethereumcontributorsusinggithubapiwithcomparativestudyofrandomgithubprofiles']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcomprehensiveanalysisofsolanaandethereumcontributorsusinggithubapiwithcomparativestudyofrandomgithubprofiles']...\n",
      "Output Row for URL_ID Netclan20241035: ['Netclan20241035', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 48.0, 0, 135.0]\n",
      "Processing row 19 (URL_ID: Netclan20241036): https://insights.blackcoffer.com/powerbi-rest-api-fetching-dataflow-and-refresh-schedules-with-seman...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercompowerbirestapifetchingdataflowandrefreshscheduleswithsemanticmodels']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercompowerbirestapifetchingdataflowandrefreshscheduleswithsemanticmodels']...\n",
      "Output Row for URL_ID Netclan20241036: ['Netclan20241036', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 94.0]\n",
      "Processing row 20 (URL_ID: Netclan20241037): https://insights.blackcoffer.com/automated-job-data-import-and-management-solution-for-enhanced-effi...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomautomatedjobdataimportandmanagementsolutionforenhancedefficiency']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomautomatedjobdataimportandmanagementsolutionforenhancedefficiency']...\n",
      "Output Row for URL_ID Netclan20241037: ['Netclan20241037', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 33.0, 0, 91.0]\n",
      "Processing row 21 (URL_ID: Netclan20241038): https://insights.blackcoffer.com/data-analytics-and-optimization-solution-for-enhancing-renewable-en...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdataanalyticsandoptimizationsolutionforenhancingrenewableenergyefficiency']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdataanalyticsandoptimizationsolutionforenhancingrenewableenergyefficiency']...\n",
      "Output Row for URL_ID Netclan20241038: ['Netclan20241038', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 36.0, 0, 100.0]\n",
      "Processing row 22 (URL_ID: Netclan20241039): https://insights.blackcoffer.com/time-series-analysis-and-trend-forecasting-solution-for-predicting-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomtimeseriesanalysisandtrendforecastingsolutionforpredictingnewstrends']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomtimeseriesanalysisandtrendforecastingsolutionforpredictingnewstrends']...\n",
      "Output Row for URL_ID Netclan20241039: ['Netclan20241039', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 30.0, 0, 95.0]\n",
      "Processing row 23 (URL_ID: Netclan20241040): https://insights.blackcoffer.com/advanced-data-visualization-solutions-for-monitoring-key-business-m...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadvanceddatavisualizationsolutionsformonitoringkeybusinessmetricswithintegratedinteractivedashboards']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadvanceddatavisualizationsolutionsformonitoringkeybusinessmetricswithintegratedinteractivedashboards']...\n",
      "Output Row for URL_ID Netclan20241040: ['Netclan20241040', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 46.0, 0, 127.0]\n",
      "Processing row 24 (URL_ID: Netclan20241041): https://insights.blackcoffer.com/advanced-patient-data-analysis-solution-for-trend-identification-an...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadvancedpatientdataanalysissolutionfortrendidentificationandimprovedhealthcareoutcome']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadvancedpatientdataanalysissolutionfortrendidentificationandimprovedhealthcareoutcome']...\n",
      "Output Row for URL_ID Netclan20241041: ['Netclan20241041', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 42.0, 0, 112.0]\n",
      "Processing row 25 (URL_ID: Netclan20241042): https://insights.blackcoffer.com/anomaly-detection-and-analysis-for-enhanced-data-integrity-and-user...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanomalydetectionandanalysisforenhanceddataintegrityanduserexperienceonbrightdataswebsite']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanomalydetectionandanalysisforenhanceddataintegrityanduserexperienceonbrightdataswebsite']...\n",
      "Output Row for URL_ID Netclan20241042: ['Netclan20241042', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 41.0, 0, 115.0]\n",
      "Processing row 26 (URL_ID: Netclan20241043): https://insights.blackcoffer.com/building-custom-tflite-models-and-benchmarking-on-voxl2-chips/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombuildingcustomtflitemodelsandbenchmarkingonvoxlchips']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombuildingcustomtflitemodelsandbenchmarkingonvoxlchips']...\n",
      "Output Row for URL_ID Netclan20241043: ['Netclan20241043', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 79.0]\n",
      "Processing row 27 (URL_ID: Netclan20241044): https://insights.blackcoffer.com/sports-prediction-model-for-multiple-sports-leagues/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomsportspredictionmodelformultiplesportsleagues']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomsportspredictionmodelformultiplesportsleagues']...\n",
      "Output Row for URL_ID Netclan20241044: ['Netclan20241044', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 72.0]\n",
      "Processing row 28 (URL_ID: Netclan20241045): https://insights.blackcoffer.com/efficient-coach-allocation-system-for-sports-coaching-organization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomefficientcoachallocationsystemforsportscoachingorganization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomefficientcoachallocationsystemforsportscoachingorganization']...\n",
      "Output Row for URL_ID Netclan20241045: ['Netclan20241045', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 86.0]\n",
      "Processing row 29 (URL_ID: Netclan20241046): https://insights.blackcoffer.com/data-studio-dashboard-with-a-data-pipeline-tool-synced-with-podio-u...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatastudiodashboardwithadatapipelinetoolsyncedwithpodiousingcustomwebhooksandgooglecloudfunction']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatastudiodashboardwithadatapipelinetoolsyncedwithpodiousingcustomwebhooksandgooglecloudfunction']...\n",
      "Output Row for URL_ID Netclan20241046: ['Netclan20241046', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 45.0, 0, 123.0]\n",
      "Processing row 30 (URL_ID: Netclan20241047): https://insights.blackcoffer.com/ai-driven-backend-for-audio-to-text-conversion-and-analytical-asses...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaidrivenbackendforaudiototextconversionandanalyticalassessmentinpharmaceuticalpractice']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaidrivenbackendforaudiototextconversionandanalyticalassessmentinpharmaceuticalpractice']...\n",
      "Output Row for URL_ID Netclan20241047: ['Netclan20241047', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 41.0, 0, 113.0]\n",
      "Processing row 31 (URL_ID: Netclan20241048): https://insights.blackcoffer.com/cloud-based-web-application-for-financial-data-processing-and-visua...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcloudbasedwebapplicationforfinancialdataprocessingandvisualizationofspmetrics']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcloudbasedwebapplicationforfinancialdataprocessingandvisualizationofspmetrics']...\n",
      "Output Row for URL_ID Netclan20241048: ['Netclan20241048', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 37.0, 0, 104.0]\n",
      "Processing row 32 (URL_ID: Netclan20241049): https://insights.blackcoffer.com/department-wise-kpi-tracking-dashboard-with-technician-performance-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdepartmentwisekpitrackingdashboardwithtechnicianperformanceanalysisforatozdependableservice']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdepartmentwisekpitrackingdashboardwithtechnicianperformanceanalysisforatozdependableservice']...\n",
      "Output Row for URL_ID Netclan20241049: ['Netclan20241049', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 39.0, 0, 118.0]\n",
      "Processing row 33 (URL_ID: Netclan20241050): https://insights.blackcoffer.com/steps-to-convert-a-node-js-api-to-python-for-aws-lambda-deployment/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomstepstoconvertanodejsapitopythonforawslambdadeployment']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomstepstoconvertanodejsapitopythonforawslambdadeployment']...\n",
      "Output Row for URL_ID Netclan20241050: ['Netclan20241050', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 24.0, 0, 81.0]\n",
      "Processing row 34 (URL_ID: Netclan20241051): https://insights.blackcoffer.com/building-an-analytics-dashboard-with-a-pdf-parsing-pipeline-for-dat...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombuildingananalyticsdashboardwithapdfparsingpipelinefordataextraction']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombuildingananalyticsdashboardwithapdfparsingpipelinefordataextraction']...\n",
      "Output Row for URL_ID Netclan20241051: ['Netclan20241051', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 95.0]\n",
      "Processing row 35 (URL_ID: Netclan20241052): https://insights.blackcoffer.com/building-a-real-time-log-file-visualization-dashboard-in-kibana/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombuildingarealtimelogfilevisualizationdashboardinkibana']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombuildingarealtimelogfilevisualizationdashboardinkibana']...\n",
      "Output Row for URL_ID Netclan20241052: ['Netclan20241052', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 81.0]\n",
      "Processing row 36 (URL_ID: Netclan20241053): https://insights.blackcoffer.com/analyzing-the-impact-of-female-ceo-appointments-on-company-stock-pr...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanalyzingtheimpactoffemaleceoappointmentsoncompanystockprices']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanalyzingtheimpactoffemaleceoappointmentsoncompanystockprices']...\n",
      "Output Row for URL_ID Netclan20241053: ['Netclan20241053', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 88.0]\n",
      "Processing row 37 (URL_ID: Netclan20241054): https://insights.blackcoffer.com/ai-chatbot-using-llm-langchain-llama/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaichatbotusingllmlangchainllama']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaichatbotusingllmlangchainllama']...\n",
      "Output Row for URL_ID Netclan20241054: ['Netclan20241054', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 17.0, 0, 58.0]\n",
      "Processing row 38 (URL_ID: Netclan20241055): https://insights.blackcoffer.com/healthcare-ai-chatbot-using-llama-llm-langchain/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomhealthcareaichatbotusingllamallmlangchain']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomhealthcareaichatbotusingllamallmlangchain']...\n",
      "Output Row for URL_ID Netclan20241055: ['Netclan20241055', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 68.0]\n",
      "Processing row 39 (URL_ID: Netclan20241056): https://insights.blackcoffer.com/ai-bot-audio-to-audio/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaibotaudiotoaudio']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaibotaudiotoaudio']...\n",
      "Output Row for URL_ID Netclan20241056: ['Netclan20241056', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 18.0, 0, 44.0]\n",
      "Processing row 40 (URL_ID: Netclan20241057): https://insights.blackcoffer.com/recommendation-engine-for-insurance-sector-to-expand-business-in-th...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomrecommendationengineforinsurancesectortoexpandbusinessintheruralarea']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomrecommendationengineforinsurancesectortoexpandbusinessintheruralarea']...\n",
      "Output Row for URL_ID Netclan20241057: ['Netclan20241057', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 35.0, 0, 95.0]\n",
      "Processing row 41 (URL_ID: Netclan20241058): https://insights.blackcoffer.com/data-from-crm-via-zapier-to-google-sheets-dynamic-to-powerbi/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatafromcrmviazapiertogooglesheetsdynamictopowerbi']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatafromcrmviazapiertogooglesheetsdynamictopowerbi']...\n",
      "Output Row for URL_ID Netclan20241058: ['Netclan20241058', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 26.0, 0, 77.0]\n",
      "Processing row 42 (URL_ID: Netclan20241059): https://insights.blackcoffer.com/data-warehouse-to-google-data-studio-looker-dashboard/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatawarehousetogoogledatastudiolookerdashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatawarehousetogoogledatastudiolookerdashboard']...\n",
      "Output Row for URL_ID Netclan20241059: ['Netclan20241059', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 73.0]\n",
      "Processing row 43 (URL_ID: Netclan20241060): https://insights.blackcoffer.com/crm-monday-com-via-zapier-to-power-bi-dashboard/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcrmmondaycomviazapiertopowerbidashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcrmmondaycomviazapiertopowerbidashboard']...\n",
      "Output Row for URL_ID Netclan20241060: ['Netclan20241060', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 66.0]\n",
      "Processing row 44 (URL_ID: Netclan20241061): https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-fr...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommondaycomtokpidashboardtomanageviewandgenerateinsightsfromthecrmdata']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommondaycomtokpidashboardtomanageviewandgenerateinsightsfromthecrmdata']...\n",
      "Output Row for URL_ID Netclan20241061: ['Netclan20241061', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 95.0]\n",
      "Processing row 45 (URL_ID: Netclan20241062): https://insights.blackcoffer.com/data-management-for-a-political-saas-application/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatamanagementforapoliticalsaasapplication']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatamanagementforapoliticalsaasapplication']...\n",
      "Output Row for URL_ID Netclan20241062: ['Netclan20241062', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 25.0, 0, 69.0]\n",
      "Processing row 46 (URL_ID: Netclan20241063): https://insights.blackcoffer.com/google-lsa-ads-google-local-service-ads-etl-tools-and-dashboards/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgooglelsaadsgooglelocalserviceadsetltoolsanddashboards']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgooglelsaadsgooglelocalserviceadsetltoolsanddashboards']...\n",
      "Output Row for URL_ID Netclan20241063: ['Netclan20241063', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 81.0]\n",
      "Processing row 47 (URL_ID: Netclan20241064): https://insights.blackcoffer.com/ad-networks-marketing-campaign-data-dashboard-in-looker-google-data...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadnetworksmarketingcampaigndatadashboardinlookergoogledatastudio']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadnetworksmarketingcampaigndatadashboardinlookergoogledatastudio']...\n",
      "Output Row for URL_ID Netclan20241064: ['Netclan20241064', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 91.0]\n",
      "Processing row 48 (URL_ID: Netclan20241065): https://insights.blackcoffer.com/analytical-solution-for-a-tech-firm/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanalyticalsolutionforatechfirm']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanalyticalsolutionforatechfirm']...\n",
      "Output Row for URL_ID Netclan20241065: ['Netclan20241065', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 18.0, 0, 57.0]\n",
      "Processing row 49 (URL_ID: Netclan20241066): https://insights.blackcoffer.com/ai-solution-for-a-technology-information-and-internet-firm/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaisolutionforatechnologyinformationandinternetfirm']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaisolutionforatechnologyinformationandinternetfirm']...\n",
      "Output Row for URL_ID Netclan20241066: ['Netclan20241066', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 77.0]\n",
      "Processing row 50 (URL_ID: Netclan20241067): https://insights.blackcoffer.com/ai-and-nlp-based-solutions-to-automate-data-discovery-for-venture-c...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaiandnlpbasedsolutionstoautomatedatadiscoveryforventurecapitalandprivateequityprincipals']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaiandnlpbasedsolutionstoautomatedatadiscoveryforventurecapitalandprivateequityprincipals']...\n",
      "Output Row for URL_ID Netclan20241067: ['Netclan20241067', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 43.0, 0, 115.0]\n",
      "Processing row 51 (URL_ID: Netclan20241068): https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanetlsolutionforaninternetpublishingfirm']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanetlsolutionforaninternetpublishingfirm']...\n",
      "Output Row for URL_ID Netclan20241068: ['Netclan20241068', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 67.0]\n",
      "Processing row 52 (URL_ID: Netclan20241069): https://insights.blackcoffer.com/ai-based-algorithmic-trading-bot-for-forex/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaibasedalgorithmictradingbotforforex']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaibasedalgorithmictradingbotforforex']...\n",
      "Output Row for URL_ID Netclan20241069: ['Netclan20241069', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 20.0, 0, 63.0]\n",
      "Processing row 53 (URL_ID: Netclan20241070): https://insights.blackcoffer.com/equity-waterfalls-model-based-saas-application-for-real-estate-sect...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomequitywaterfallsmodelbasedsaasapplicationforrealestatesector']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomequitywaterfallsmodelbasedsaasapplicationforrealestatesector']...\n",
      "Output Row for URL_ID Netclan20241070: ['Netclan20241070', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 87.0]\n",
      "Processing row 54 (URL_ID: Netclan20241071): https://insights.blackcoffer.com/ai-solutions-for-foreign-exchange-an-automated-algo-trading-tool/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaisolutionsforforeignexchangeanautomatedalgotradingtool']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaisolutionsforforeignexchangeanautomatedalgotradingtool']...\n",
      "Output Row for URL_ID Netclan20241071: ['Netclan20241071', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 82.0]\n",
      "Processing row 55 (URL_ID: Netclan20241072): https://insights.blackcoffer.com/ai-agent-development-and-deployment-in-jina-ai/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaiagentdevelopmentanddeploymentinjinaai']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaiagentdevelopmentanddeploymentinjinaai']...\n",
      "Output Row for URL_ID Netclan20241072: ['Netclan20241072', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 66.0]\n",
      "Processing row 56 (URL_ID: Netclan20241073): https://insights.blackcoffer.com/golden-record-a-knowledge-graph-database-approach-to-unfold-discove...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgoldenrecordaknowledgegraphdatabaseapproachtounfolddiscoveryusingneoj']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgoldenrecordaknowledgegraphdatabaseapproachtounfolddiscoveryusingneoj']...\n",
      "Output Row for URL_ID Netclan20241073: ['Netclan20241073', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 96.0]\n",
      "Processing row 57 (URL_ID: Netclan20241074): https://insights.blackcoffer.com/advanced-ai-for-trading-automation/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadvancedaifortradingautomation']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadvancedaifortradingautomation']...\n",
      "Output Row for URL_ID Netclan20241074: ['Netclan20241074', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 20.0, 0, 57.0]\n",
      "Processing row 58 (URL_ID: Netclan20241075): https://insights.blackcoffer.com/create-a-knowledge-graph-to-provide-real-time-analytics-recommendat...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcreateaknowledgegraphtoproviderealtimeanalyticsrecommendationsandasinglesourceoftruth']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcreateaknowledgegraphtoproviderealtimeanalyticsrecommendationsandasinglesourceoftruth']...\n",
      "Output Row for URL_ID Netclan20241075: ['Netclan20241075', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 40.0, 0, 112.0]\n",
      "Processing row 59 (URL_ID: Netclan20241076): https://insights.blackcoffer.com/advanced-ai-for-thermal-person-detection/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadvancedaiforthermalpersondetection']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadvancedaiforthermalpersondetection']...\n",
      "Output Row for URL_ID Netclan20241076: ['Netclan20241076', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 20.0, 0, 62.0]\n",
      "Processing row 60 (URL_ID: Netclan20241077): https://insights.blackcoffer.com/advanced-ai-for-road-cam-threat-detection/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadvancedaiforroadcamthreatdetection']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadvancedaiforroadcamthreatdetection']...\n",
      "Output Row for URL_ID Netclan20241077: ['Netclan20241077', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 62.0]\n",
      "Processing row 61 (URL_ID: Netclan20241078): https://insights.blackcoffer.com/advanced-ai-for-pedestrian-crossing-safety/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomadvancedaiforpedestriancrossingsafety']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomadvancedaiforpedestriancrossingsafety']...\n",
      "Output Row for URL_ID Netclan20241078: ['Netclan20241078', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 20.0, 0, 64.0]\n",
      "Processing row 62 (URL_ID: Netclan20241079): https://insights.blackcoffer.com/handgun-detection-using-yolo/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomhandgundetectionusingyolo']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomhandgundetectionusingyolo']...\n",
      "Output Row for URL_ID Netclan20241079: ['Netclan20241079', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 16.0, 0, 52.0]\n",
      "Processing row 63 (URL_ID: Netclan20241080): https://insights.blackcoffer.com/using-graph-technology-to-create-single-customer-view/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomusinggraphtechnologytocreatesinglecustomerview']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomusinggraphtechnologytocreatesinglecustomerview']...\n",
      "Output Row for URL_ID Netclan20241080: ['Netclan20241080', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 73.0]\n",
      "Processing row 64 (URL_ID: Netclan20241081): https://insights.blackcoffer.com/car-detection-in-satellite-images/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcardetectioninsatelliteimages']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcardetectioninsatelliteimages']...\n",
      "Output Row for URL_ID Netclan20241081: ['Netclan20241081', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 19.0, 0, 56.0]\n",
      "Processing row 65 (URL_ID: Netclan20241082): https://insights.blackcoffer.com/building-a-physics-informed-neural-network-for-circuit-evaluation/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombuildingaphysicsinformedneuralnetworkforcircuitevaluation']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombuildingaphysicsinformedneuralnetworkforcircuitevaluation']...\n",
      "Output Row for URL_ID Netclan20241082: ['Netclan20241082', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 84.0]\n",
      "Processing row 66 (URL_ID: Netclan20241083): https://insights.blackcoffer.com/connecting-mongodb-database-to-power-bi-dashboard-dashboard-automat...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomconnectingmongodbdatabasetopowerbidashboarddashboardautomation']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomconnectingmongodbdatabasetopowerbidashboarddashboardautomation']...\n",
      "Output Row for URL_ID Netclan20241083: ['Netclan20241083', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 89.0]\n",
      "Processing row 67 (URL_ID: Netclan20241084): https://insights.blackcoffer.com/data-transformation/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatatransformation']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatatransformation']...\n",
      "Output Row for URL_ID Netclan20241084: ['Netclan20241084', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 13.0, 0, 45.0]\n",
      "Processing row 68 (URL_ID: Netclan20241085): https://insights.blackcoffer.com/e-commerce-store-analysis-purchase-behavior-ad-spend-conversion-tra...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomecommercestoreanalysispurchasebehavioradspendconversiontrafficetc']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomecommercestoreanalysispurchasebehavioradspendconversiontrafficetc']...\n",
      "Output Row for URL_ID Netclan20241085: ['Netclan20241085', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 92.0]\n",
      "Processing row 69 (URL_ID: Netclan20241086): https://insights.blackcoffer.com/kpi-dashboard-for-accountants/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomkpidashboardforaccountants']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomkpidashboardforaccountants']...\n",
      "Output Row for URL_ID Netclan20241086: ['Netclan20241086', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 15.0, 0, 53.0]\n",
      "Processing row 70 (URL_ID: Netclan20241087): https://insights.blackcoffer.com/return-on-advertising-spend-dashboard-marketing-automation-and-anal...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomreturnonadvertisingspenddashboardmarketingautomationandanalyticsusingetlanddashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomreturnonadvertisingspenddashboardmarketingautomationandanalyticsusingetlanddashboard']...\n",
      "Output Row for URL_ID Netclan20241087: ['Netclan20241087', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 37.0, 0, 111.0]\n",
      "Processing row 71 (URL_ID: Netclan20241088): https://insights.blackcoffer.com/ranking-customer-behaviours-for-business-strategy/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomrankingcustomerbehavioursforbusinessstrategy']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomrankingcustomerbehavioursforbusinessstrategy']...\n",
      "Output Row for URL_ID Netclan20241088: ['Netclan20241088', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 71.0]\n",
      "Processing row 72 (URL_ID: Netclan20241089): https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-met...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomalgorithmictradingformultiplecommoditiesmarketslikeforexmetalsenergyetc']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomalgorithmictradingformultiplecommoditiesmarketslikeforexmetalsenergyetc']...\n",
      "Output Row for URL_ID Netclan20241089: ['Netclan20241089', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 98.0]\n",
      "Processing row 73 (URL_ID: Netclan20241090): https://insights.blackcoffer.com/trading-bot-for-forex/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomtradingbotforforex']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomtradingbotforforex']...\n",
      "Output Row for URL_ID Netclan20241090: ['Netclan20241090', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 12.0, 0, 45.0]\n",
      "Processing row 74 (URL_ID: Netclan20241091): https://insights.blackcoffer.com/python-model-for-the-analysis-of-sector-specific-stock-etfs-for-inv...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercompythonmodelfortheanalysisofsectorspecificstocketfsforinvestmentpurposesefbfbc']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercompythonmodelfortheanalysisofsectorspecificstocketfsforinvestmentpurposesefbfbc']...\n",
      "Output Row for URL_ID Netclan20241091: ['Netclan20241091', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 30.0, 0, 104.0]\n",
      "Processing row 75 (URL_ID: Netclan20241092): https://insights.blackcoffer.com/medical-classification/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommedicalclassification']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommedicalclassification']...\n",
      "Output Row for URL_ID Netclan20241092: ['Netclan20241092', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 15.0, 0, 48.0]\n",
      "Processing row 76 (URL_ID: Netclan20241093): https://insights.blackcoffer.com/design-develop-bert-question-answering-model-explanations-with-visu...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdesigndevelopbertquestionansweringmodelexplanationswithvisualization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdesigndevelopbertquestionansweringmodelexplanationswithvisualization']...\n",
      "Output Row for URL_ID Netclan20241093: ['Netclan20241093', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 34.0, 0, 95.0]\n",
      "Processing row 77 (URL_ID: Netclan20241094): https://insights.blackcoffer.com/design-and-develop-solution-to-anomaly-detection-classification-pro...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdesignanddevelopsolutiontoanomalydetectionclassificationproblems']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdesignanddevelopsolutiontoanomalydetectionclassificationproblems']...\n",
      "Output Row for URL_ID Netclan20241094: ['Netclan20241094', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 91.0]\n",
      "Processing row 78 (URL_ID: Netclan20241095): https://insights.blackcoffer.com/an-etl-solution-for-currency-data-to-google-big-query/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanetlsolutionforcurrencydatatogooglebigquery']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanetlsolutionforcurrencydatatogooglebigquery']...\n",
      "Output Row for URL_ID Netclan20241095: ['Netclan20241095', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 24.0, 0, 71.0]\n",
      "Processing row 79 (URL_ID: Netclan20241096): https://insights.blackcoffer.com/etl-and-mlops-infrastructure-for-blockchain-analytics/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercometlandmlopsinfrastructureforblockchainanalytics']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercometlandmlopsinfrastructureforblockchainanalytics']...\n",
      "Output Row for URL_ID Netclan20241096: ['Netclan20241096', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 74.0]\n",
      "Processing row 80 (URL_ID: Netclan20241097): https://insights.blackcoffer.com/an-agent-based-model-of-a-virtual-power-plant-vpp/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanagentbasedmodelofavirtualpowerplantvpp']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanagentbasedmodelofavirtualpowerplantvpp']...\n",
      "Output Row for URL_ID Netclan20241097: ['Netclan20241097', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 67.0]\n",
      "Processing row 81 (URL_ID: Netclan20241098): https://insights.blackcoffer.com/transform-api-into-sdk-library-and-widget/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomtransformapiintosdklibraryandwidget']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomtransformapiintosdklibraryandwidget']...\n",
      "Output Row for URL_ID Netclan20241098: ['Netclan20241098', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 17.0, 0, 62.0]\n",
      "Processing row 82 (URL_ID: Netclan20241099): https://insights.blackcoffer.com/integration-of-a-product-to-a-cloud-based-crm-platform/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomintegrationofaproducttoacloudbasedcrmplatform']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomintegrationofaproducttoacloudbasedcrmplatform']...\n",
      "Output Row for URL_ID Netclan20241099: ['Netclan20241099', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 72.0]\n",
      "Processing row 83 (URL_ID: Netclan20241100): https://insights.blackcoffer.com/a-web-based-dashboard-for-the-filtered-data-retrieval-of-land-recor...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomawebbaseddashboardforthefiltereddataretrievaloflandrecords']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomawebbaseddashboardforthefiltereddataretrievaloflandrecords']...\n",
      "Output Row for URL_ID Netclan20241100: ['Netclan20241100', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 85.0]\n",
      "Processing row 84 (URL_ID: Netclan20241101): https://insights.blackcoffer.com/integration-of-video-conferencing-data-to-the-existing-web-app/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomintegrationofvideoconferencingdatatotheexistingwebapp']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomintegrationofvideoconferencingdatatotheexistingwebapp']...\n",
      "Output Row for URL_ID Netclan20241101: ['Netclan20241101', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 80.0]\n",
      "Processing row 85 (URL_ID: Netclan20241102): https://insights.blackcoffer.com/design-develop-an-app-in-retool-which-shows-the-progress-of-the-add...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdesigndevelopanappinretoolwhichshowstheprogressoftheaddedvideo']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdesigndevelopanappinretoolwhichshowstheprogressoftheaddedvideo']...\n",
      "Output Row for URL_ID Netclan20241102: ['Netclan20241102', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 89.0]\n",
      "Processing row 86 (URL_ID: Netclan20241103): https://insights.blackcoffer.com/auvik-connectwise-integration-in-grafana/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomauvikconnectwiseintegrationingrafana']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomauvikconnectwiseintegrationingrafana']...\n",
      "Output Row for URL_ID Netclan20241103: ['Netclan20241103', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 63.0]\n",
      "Processing row 87 (URL_ID: Netclan20241104): https://insights.blackcoffer.com/data-integration-and-big-data-performance-using-elk-stack/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdataintegrationandbigdataperformanceusingelkstack']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdataintegrationandbigdataperformanceusingelkstack']...\n",
      "Output Row for URL_ID Netclan20241104: ['Netclan20241104', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 25.0, 0, 76.0]\n",
      "Processing row 88 (URL_ID: Netclan20241105): https://insights.blackcoffer.com/web-data-connector/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomwebdataconnector']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomwebdataconnector']...\n",
      "Output Row for URL_ID Netclan20241105: ['Netclan20241105', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 12.0, 0, 43.0]\n",
      "Processing row 89 (URL_ID: Netclan20241106): https://insights.blackcoffer.com/an-app-for-updating-the-email-id-of-the-user-and-stripe-refund-tool...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanappforupdatingtheemailidoftheuserandstriperefundtoolusingretool']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanappforupdatingtheemailidoftheuserandstriperefundtoolusingretool']...\n",
      "Output Row for URL_ID Netclan20241106: ['Netclan20241106', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 33.0, 0, 92.0]\n",
      "Processing row 90 (URL_ID: Netclan20241107): https://insights.blackcoffer.com/an-ai-ml-based-web-application-that-detects-the-correctness-of-text...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomanaimlbasedwebapplicationthatdetectsthecorrectnessoftextinagivenvideo']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomanaimlbasedwebapplicationthatdetectsthecorrectnessoftextinagivenvideo']...\n",
      "Output Row for URL_ID Netclan20241107: ['Netclan20241107', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 33.0, 0, 96.0]\n",
      "Processing row 91 (URL_ID: Netclan20241108): https://insights.blackcoffer.com/website-tracking-and-insights-using-google-analytics-google-tag-man...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomwebsitetrackingandinsightsusinggoogleanalyticsgoogletagmanager']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomwebsitetrackingandinsightsusinggoogleanalyticsgoogletagmanager']...\n",
      "Output Row for URL_ID Netclan20241108: ['Netclan20241108', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 89.0]\n",
      "Processing row 92 (URL_ID: Netclan20241109): https://insights.blackcoffer.com/dashboard-to-track-the-analytics-of-the-website-using-google-analyt...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdashboardtotracktheanalyticsofthewebsiteusinggoogleanalyticsandgoogletagmanager']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdashboardtotracktheanalyticsofthewebsiteusinggoogleanalyticsandgoogletagmanager']...\n",
      "Output Row for URL_ID Netclan20241109: ['Netclan20241109', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 36.0, 0, 106.0]\n",
      "Processing row 93 (URL_ID: Netclan20241110): https://insights.blackcoffer.com/power-bi-dashboard-on-operations-transactions-and-marketing-embeddi...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercompowerbidashboardonoperationstransactionsandmarketingembeddingthedashboardtowebapp']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercompowerbidashboardonoperationstransactionsandmarketingembeddingthedashboardtowebapp']...\n",
      "Output Row for URL_ID Netclan20241110: ['Netclan20241110', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 36.0, 0, 108.0]\n",
      "Processing row 94 (URL_ID: Netclan20241111): https://insights.blackcoffer.com/nft-data-automation-looksrare-and-etl-tool/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomnftdataautomationlooksrareandetltool']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomnftdataautomationlooksrareandetltool']...\n",
      "Output Row for URL_ID Netclan20241111: ['Netclan20241111', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 63.0]\n",
      "Processing row 95 (URL_ID: Netclan20241112): https://insights.blackcoffer.com/optimize-the-data-scraper-program-to-easily-accommodate-large-files...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomoptimizethedatascraperprogramtoeasilyaccommodatelargefilesandsolveoomerrors']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomoptimizethedatascraperprogramtoeasilyaccommodatelargefilesandsolveoomerrors']...\n",
      "Output Row for URL_ID Netclan20241112: ['Netclan20241112', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 37.0, 0, 102.0]\n",
      "Processing row 96 (URL_ID: Netclan20241113): https://insights.blackcoffer.com/making-a-robust-way-to-sync-data-from-airtables-to-mongodb-using-py...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommakingarobustwaytosyncdatafromairtablestomongodbusingpythonetlsolution']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommakingarobustwaytosyncdatafromairtablestomongodbusingpythonetlsolution']...\n",
      "Output Row for URL_ID Netclan20241113: ['Netclan20241113', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 97.0]\n",
      "Processing row 97 (URL_ID: Netclan20241114): https://insights.blackcoffer.com/incident-duration-prediction-infrastructure-and-real-estate/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomincidentdurationpredictioninfrastructureandrealestate']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomincidentdurationpredictioninfrastructureandrealestate']...\n",
      "Output Row for URL_ID Netclan20241114: ['Netclan20241114', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 80.0]\n",
      "Processing row 98 (URL_ID: Netclan20241115): https://insights.blackcoffer.com/statistical-data-analysis-of-reinforced-concrete/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomstatisticaldataanalysisofreinforcedconcrete']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomstatisticaldataanalysisofreinforcedconcrete']...\n",
      "Output Row for URL_ID Netclan20241115: ['Netclan20241115', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 70.0]\n",
      "Processing row 99 (URL_ID: Netclan20241116): https://insights.blackcoffer.com/database-normalization-segmentation-with-google-data-studio-dashboa...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatabasenormalizationsegmentationwithgoogledatastudiodashboardinsights']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatabasenormalizationsegmentationwithgoogledatastudiodashboardinsights']...\n",
      "Output Row for URL_ID Netclan20241116: ['Netclan20241116', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 35.0, 0, 97.0]\n",
      "Processing row 100 (URL_ID: Netclan20241117): https://insights.blackcoffer.com/power-bi-dashboard-to-drive-insights-from-complex-data-to-generate-...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercompowerbidashboardtodriveinsightsfromcomplexdatatogeneratebusinessinsights']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercompowerbidashboardtodriveinsightsfromcomplexdatatogeneratebusinessinsights']...\n",
      "Output Row for URL_ID Netclan20241117: ['Netclan20241117', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 99.0]\n",
      "Processing row 101 (URL_ID: Netclan20241118): https://insights.blackcoffer.com/real-time-dashboard-to-monitor-infrastructure-activity-and-machines...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomrealtimedashboardtomonitorinfrastructureactivityandmachines']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomrealtimedashboardtomonitorinfrastructureactivityandmachines']...\n",
      "Output Row for URL_ID Netclan20241118: ['Netclan20241118', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 86.0]\n",
      "Processing row 102 (URL_ID: Netclan20241119): https://insights.blackcoffer.com/electric-vehicles-ev-load-management-system-to-forecast-energy-dema...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomelectricvehiclesevloadmanagementsystemtoforecastenergydemand']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomelectricvehiclesevloadmanagementsystemtoforecastenergydemand']...\n",
      "Output Row for URL_ID Netclan20241119: ['Netclan20241119', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 87.0]\n",
      "Processing row 103 (URL_ID: Netclan20241120): https://insights.blackcoffer.com/power-bi-data-driven-map-dashboard/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercompowerbidatadrivenmapdashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercompowerbidatadrivenmapdashboard']...\n",
      "Output Row for URL_ID Netclan20241120: ['Netclan20241120', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 17.0, 0, 56.0]\n",
      "Processing row 104 (URL_ID: Netclan20241121): https://insights.blackcoffer.com/google-local-service-ads-lsa-leads-dashboard/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgooglelocalserviceadslsaleadsdashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgooglelocalserviceadslsaleadsdashboard']...\n",
      "Output Row for URL_ID Netclan20241121: ['Netclan20241121', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 65.0]\n",
      "Processing row 105 (URL_ID: Netclan20241122): https://insights.blackcoffer.com/aws-lex-voice-and-chatbot/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomawslexvoiceandchatbot']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomawslexvoiceandchatbot']...\n",
      "Output Row for URL_ID Netclan20241122: ['Netclan20241122', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 14.0, 0, 48.0]\n",
      "Processing row 106 (URL_ID: Netclan20241123): https://insights.blackcoffer.com/metabridges-api-decentraland-integration/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommetabridgesapidecentralandintegration']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommetabridgesapidecentralandintegration']...\n",
      "Output Row for URL_ID Netclan20241123: ['Netclan20241123', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 64.0]\n",
      "Processing row 107 (URL_ID: Netclan20241124): https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommicrosoftazurechatbotwithluislanguageunderstanding']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommicrosoftazurechatbotwithluislanguageunderstanding']...\n",
      "Output Row for URL_ID Netclan20241124: ['Netclan20241124', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 25.0, 0, 77.0]\n",
      "Processing row 108 (URL_ID: Netclan20241125): https://insights.blackcoffer.com/impact-of-news-media-and-press-on-innovation-startups-and-investmen...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomimpactofnewsmediaandpressoninnovationstartupsandinvestments']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomimpactofnewsmediaandpressoninnovationstartupsandinvestments']...\n",
      "Output Row for URL_ID Netclan20241125: ['Netclan20241125', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 86.0]\n",
      "Processing row 109 (URL_ID: Netclan20241126): https://insights.blackcoffer.com/aws-quicksight-reporting-dashboard/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomawsquicksightreportingdashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomawsquicksightreportingdashboard']...\n",
      "Output Row for URL_ID Netclan20241126: ['Netclan20241126', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 16.0, 0, 58.0]\n",
      "Processing row 110 (URL_ID: Netclan20241127): https://insights.blackcoffer.com/google-data-studio-dashboard-for-marketing-ads-and-traction-data/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgoogledatastudiodashboardformarketingadsandtractiondata']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgoogledatastudiodashboardformarketingadsandtractiondata']...\n",
      "Output Row for URL_ID Netclan20241127: ['Netclan20241127', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 82.0]\n",
      "Processing row 111 (URL_ID: Netclan20241128): https://insights.blackcoffer.com/gangala-in-e-commerce-big-data-etl-elt-solution-and-data-warehouse/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgangalainecommercebigdataetleltsolutionanddatawarehouse']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgangalainecommercebigdataetleltsolutionanddatawarehouse']...\n",
      "Output Row for URL_ID Netclan20241128: ['Netclan20241128', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 31.0, 0, 82.0]\n",
      "Processing row 112 (URL_ID: Netclan20241129): https://insights.blackcoffer.com/big-data-solution-to-an-online-multivendor-marketplace-ecommerce-bu...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombigdatasolutiontoanonlinemultivendormarketplaceecommercebusiness']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombigdatasolutiontoanonlinemultivendormarketplaceecommercebusiness']...\n",
      "Output Row for URL_ID Netclan20241129: ['Netclan20241129', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 33.0, 0, 91.0]\n",
      "Processing row 113 (URL_ID: Netclan20241130): https://insights.blackcoffer.com/creating-a-custom-report-and-dashboard-using-the-data-got-from-ater...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcreatingacustomreportanddashboardusingthedatagotfromateraapi']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcreatingacustomreportanddashboardusingthedatagotfromateraapi']...\n",
      "Output Row for URL_ID Netclan20241130: ['Netclan20241130', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 30.0, 0, 87.0]\n",
      "Processing row 114 (URL_ID: Netclan20241131): https://insights.blackcoffer.com/azure-data-lake-and-power-bi-dashboard/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomazuredatalakeandpowerbidashboard']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomazuredatalakeandpowerbidashboard']...\n",
      "Output Row for URL_ID Netclan20241131: ['Netclan20241131', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 20.0, 0, 59.0]\n",
      "Processing row 115 (URL_ID: Netclan20241132): https://insights.blackcoffer.com/google-data-studio-pipeline-with-gcp-mysql/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgoogledatastudiopipelinewithgcpmysql']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgoogledatastudiopipelinewithgcpmysql']...\n",
      "Output Row for URL_ID Netclan20241132: ['Netclan20241132', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 19.0, 0, 63.0]\n",
      "Processing row 116 (URL_ID: Netclan20241133): https://insights.blackcoffer.com/quickbooks-dashboard-to-find-patterns-in-finance-sales-and-forecast...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomquickbooksdashboardtofindpatternsinfinancesalesandforecasts']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomquickbooksdashboardtofindpatternsinfinancesalesandforecasts']...\n",
      "Output Row for URL_ID Netclan20241133: ['Netclan20241133', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 86.0]\n",
      "Processing row 117 (URL_ID: Netclan20241134): https://insights.blackcoffer.com/marketing-sales-and-financial-data-business-dashboard-wink-report/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommarketingsalesandfinancialdatabusinessdashboardwinkreport']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommarketingsalesandfinancialdatabusinessdashboardwinkreport']...\n",
      "Output Row for URL_ID Netclan20241134: ['Netclan20241134', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 84.0]\n",
      "Processing row 118 (URL_ID: Netclan20241135): https://insights.blackcoffer.com/react-native-apps-in-the-development-portfolio/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomreactnativeappsinthedevelopmentportfolio']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomreactnativeappsinthedevelopmentportfolio']...\n",
      "Output Row for URL_ID Netclan20241135: ['Netclan20241135', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 67.0]\n",
      "Processing row 119 (URL_ID: Netclan20241136): https://insights.blackcoffer.com/a-leading-firm-website-seo-optimization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaleadingfirmwebsiteseooptimization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaleadingfirmwebsiteseooptimization']...\n",
      "Output Row for URL_ID Netclan20241136: ['Netclan20241136', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 61.0]\n",
      "Processing row 120 (URL_ID: Netclan20241137): https://insights.blackcoffer.com/a-leading-hospitality-firm-in-the-usa-website-seo-optimization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaleadinghospitalityfirmintheusawebsiteseooptimization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaleadinghospitalityfirmintheusawebsiteseooptimization']...\n",
      "Output Row for URL_ID Netclan20241137: ['Netclan20241137', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 30.0, 0, 80.0]\n",
      "Processing row 121 (URL_ID: Netclan20241138): https://insights.blackcoffer.com/a-leading-firm-in-the-usa-website-seo-optimization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaleadingfirmintheusawebsiteseooptimization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaleadingfirmintheusawebsiteseooptimization']...\n",
      "Output Row for URL_ID Netclan20241138: ['Netclan20241138', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 26.0, 0, 69.0]\n",
      "Processing row 122 (URL_ID: Netclan20241139): https://insights.blackcoffer.com/a-leading-musical-instrumental-website-seo-optimization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaleadingmusicalinstrumentalwebsiteseooptimization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaleadingmusicalinstrumentalwebsiteseooptimization']...\n",
      "Output Row for URL_ID Netclan20241139: ['Netclan20241139', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 76.0]\n",
      "Processing row 123 (URL_ID: Netclan20241140): https://insights.blackcoffer.com/a-leading-firm-in-the-usa-seo-and-website-optimization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaleadingfirmintheusaseoandwebsiteoptimization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaleadingfirmintheusaseoandwebsiteoptimization']...\n",
      "Output Row for URL_ID Netclan20241140: ['Netclan20241140', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 72.0]\n",
      "Processing row 124 (URL_ID: Netclan20241141): https://insights.blackcoffer.com/immigration-datawarehouse-ai-based-recommendations/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomimmigrationdatawarehouseaibasedrecommendations']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomimmigrationdatawarehouseaibasedrecommendations']...\n",
      "Output Row for URL_ID Netclan20241141: ['Netclan20241141', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 28.0, 0, 73.0]\n",
      "Processing row 125 (URL_ID: Netclan20241142): https://insights.blackcoffer.com/lipsync-automation-for-celebrities-and-influencers/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomlipsyncautomationforcelebritiesandinfluencers']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomlipsyncautomationforcelebritiesandinfluencers']...\n",
      "Output Row for URL_ID Netclan20241142: ['Netclan20241142', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 24.0, 0, 72.0]\n",
      "Processing row 126 (URL_ID: Netclan20241143): https://insights.blackcoffer.com/key-audit-matters-predictive-modeling/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomkeyauditmatterspredictivemodeling']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomkeyauditmatterspredictivemodeling']...\n",
      "Output Row for URL_ID Netclan20241143: ['Netclan20241143', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 19.0, 0, 60.0]\n",
      "Processing row 127 (URL_ID: Netclan20241144): https://insights.blackcoffer.com/splitting-of-songs-into-its-vocals-and-instrumental/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomsplittingofsongsintoitsvocalsandinstrumental']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomsplittingofsongsintoitsvocalsandinstrumental']...\n",
      "Output Row for URL_ID Netclan20241144: ['Netclan20241144', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 20.0, 0, 71.0]\n",
      "Processing row 128 (URL_ID: Netclan20241145): https://insights.blackcoffer.com/ai-and-ml-technologies-to-evaluate-learning-assessments/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomaiandmltechnologiestoevaluatelearningassessments']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomaiandmltechnologiestoevaluatelearningassessments']...\n",
      "Output Row for URL_ID Netclan20241145: ['Netclan20241145', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 26.0, 0, 75.0]\n",
      "Processing row 129 (URL_ID: Netclan20241146): https://insights.blackcoffer.com/datawarehouse-and-recommendations-engine-for-airbnb/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdatawarehouseandrecommendationsengineforairbnb']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdatawarehouseandrecommendationsengineforairbnb']...\n",
      "Output Row for URL_ID Netclan20241146: ['Netclan20241146', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 26.0, 0, 73.0]\n",
      "Processing row 130 (URL_ID: Netclan20241147): https://insights.blackcoffer.com/real-estate-data-warehouse/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomrealestatedatawarehouse']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomrealestatedatawarehouse']...\n",
      "Output Row for URL_ID Netclan20241147: ['Netclan20241147', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 18.0, 0, 50.0]\n",
      "Processing row 131 (URL_ID: Netclan20241148): https://insights.blackcoffer.com/traction-dashboards-of-marketing-campaigns-and-posts/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomtractiondashboardsofmarketingcampaignsandposts']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomtractiondashboardsofmarketingcampaignsandposts']...\n",
      "Output Row for URL_ID Netclan20241148: ['Netclan20241148', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 73.0]\n",
      "Processing row 132 (URL_ID: Netclan20241149): https://insights.blackcoffer.com/google-local-service-ads-lsa-data-warehouse/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgooglelocalserviceadslsadatawarehouse']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgooglelocalserviceadslsadatawarehouse']...\n",
      "Output Row for URL_ID Netclan20241149: ['Netclan20241149', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 64.0]\n",
      "Processing row 133 (URL_ID: Netclan20241150): https://insights.blackcoffer.com/google-local-service-ads-missed-calls-and-messages-automation-tool/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgooglelocalserviceadsmissedcallsandmessagesautomationtool']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgooglelocalserviceadsmissedcallsandmessagesautomationtool']...\n",
      "Output Row for URL_ID Netclan20241150: ['Netclan20241150', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 30.0, 0, 84.0]\n",
      "Processing row 134 (URL_ID: Netclan20241151): https://insights.blackcoffer.com/marketing-ads-leads-call-status-data-tool-to-bigquery/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommarketingadsleadscallstatusdatatooltobigquery']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommarketingadsleadscallstatusdatatooltobigquery']...\n",
      "Output Row for URL_ID Netclan20241151: ['Netclan20241151', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 72.0]\n",
      "Processing row 135 (URL_ID: Netclan20241152): https://insights.blackcoffer.com/marketing-analytics-to-automate-leads-call-status-and-reporting/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommarketinganalyticstoautomateleadscallstatusandreporting']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommarketinganalyticstoautomateleadscallstatusandreporting']...\n",
      "Output Row for URL_ID Netclan20241152: ['Netclan20241152', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 27.0, 0, 82.0]\n",
      "Processing row 136 (URL_ID: Netclan20241153): https://insights.blackcoffer.com/callrail-analytics-leads-report-alert/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomcallrailanalyticsleadsreportalert']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomcallrailanalyticsleadsreportalert']...\n",
      "Output Row for URL_ID Netclan20241153: ['Netclan20241153', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 18.0, 0, 60.0]\n",
      "Processing row 137 (URL_ID: Netclan20241154): https://insights.blackcoffer.com/marketing-automation-tool-to-notify-lead-details-to-clients-over-em...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommarketingautomationtooltonotifyleaddetailstoclientsoveremailandphone']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommarketingautomationtooltonotifyleaddetailstoclientsoveremailandphone']...\n",
      "Output Row for URL_ID Netclan20241154: ['Netclan20241154', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 36.0, 0, 95.0]\n",
      "Processing row 138 (URL_ID: Netclan20241155): https://insights.blackcoffer.com/data-etl-local-service-ads-leads-to-bigquery/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomdataetllocalserviceadsleadstobigquery']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomdataetllocalserviceadsleadstobigquery']...\n",
      "Output Row for URL_ID Netclan20241155: ['Netclan20241155', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 21.0, 0, 64.0]\n",
      "Processing row 139 (URL_ID: Netclan20241156): https://insights.blackcoffer.com/marbles-stimulation-using-python/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercommarblesstimulationusingpython']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercommarblesstimulationusingpython']...\n",
      "Output Row for URL_ID Netclan20241156: ['Netclan20241156', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 16.0, 0, 56.0]\n",
      "Processing row 140 (URL_ID: Netclan20241157): https://insights.blackcoffer.com/stocktwits-data-structurization/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomstocktwitsdatastructurization']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomstocktwitsdatastructurization']...\n",
      "Output Row for URL_ID Netclan20241157: ['Netclan20241157', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 16.0, 0, 56.0]\n",
      "Processing row 141 (URL_ID: Netclan20241158): https://insights.blackcoffer.com/sentimental-analysis-on-shareholder-letter-of-companies/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomsentimentalanalysisonshareholderletterofcompanies']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomsentimentalanalysisonshareholderletterofcompanies']...\n",
      "Output Row for URL_ID Netclan20241158: ['Netclan20241158', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 25.0, 0, 76.0]\n",
      "Processing row 142 (URL_ID: Netclan20241159): https://insights.blackcoffer.com/population-and-community-survey-of-america/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercompopulationandcommunitysurveyofamerica']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercompopulationandcommunitysurveyofamerica']...\n",
      "Output Row for URL_ID Netclan20241159: ['Netclan20241159', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 22.0, 0, 64.0]\n",
      "Processing row 143 (URL_ID: Netclan20241160): https://insights.blackcoffer.com/google-lsa-api-data-automation-and-dashboarding/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomgooglelsaapidataautomationanddashboarding']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomgooglelsaapidataautomationanddashboarding']...\n",
      "Output Row for URL_ID Netclan20241160: ['Netclan20241160', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 25.0, 0, 68.0]\n",
      "Processing row 144 (URL_ID: Netclan20241161): https://insights.blackcoffer.com/healthcare-data-analysis/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomhealthcaredataanalysis']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomhealthcaredataanalysis']...\n",
      "Output Row for URL_ID Netclan20241161: ['Netclan20241161', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 15.0, 0, 49.0]\n",
      "Processing row 145 (URL_ID: Netclan20241162): https://insights.blackcoffer.com/budget-sales-kpi-dashboard-using-power-bi/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercombudgetsaleskpidashboardusingpowerbi']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercombudgetsaleskpidashboardusingpowerbi']...\n",
      "Output Row for URL_ID Netclan20241162: ['Netclan20241162', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 19.0, 0, 62.0]\n",
      "Processing row 146 (URL_ID: Netclan20241163): https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/...\n",
      "Cleaned Text: ['httpsinsightsblackcoffercomamazonbuybotanautomationaitooltoautocheckouts']...\n",
      "Positive Score: 0, Negative Score: 0, Polarity Score: 0.0\n",
      "Complex Words: ['httpsinsightsblackcoffercomamazonbuybotanautomationaitooltoautocheckouts']...\n",
      "Output Row for URL_ID Netclan20241163: ['Netclan20241163', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 29.0, 0, 72.0]\n",
      "First few rows of output data:\n",
      "[['Netclan20241017', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 44.0, 0, 128.0], ['Netclan20241018', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 50.0, 0, 141.0], ['Netclan20241019', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 32.0, 0, 94.0], ['Netclan20241020', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 62.0, 0, 177.0], ['Netclan20241021', 0, 0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5333333333333333, 0.3333333333333333, 1, 1, 23.0, 0, 66.0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "15 columns passed, passed data had 14 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 15 columns passed, passed data had 14 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[204], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_data[:\u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Print first 5 rows of output data\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Save results to an Excel file\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(output_data, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[0;32m     41\u001b[0m output_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzed_Output.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData processed and saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    854\u001b[0m         data,\n\u001b[0;32m    855\u001b[0m         columns,\n\u001b[0;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    857\u001b[0m         dtype,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 15 columns passed, passed data had 14 columns"
     ]
    }
   ],
   "source": [
    "# Process all input text data\n",
    "output_data = []\n",
    "for index, row in data.iterrows():\n",
    "    text = row['URL']  # Use 'URL' as the column containing the text\n",
    "    \n",
    "    # Print the row data being processed\n",
    "    print(f\"Processing row {index} (URL_ID: {row['URL_ID']}): {text[:100]}...\")  # Preview first 100 characters\n",
    "    \n",
    "    # Clean and analyze the text\n",
    "    words = clean_text(text)\n",
    "    print(f\"Cleaned Text: {words[:20]}...\")  # Preview first 20 words after cleaning\n",
    "    \n",
    "    # Analyze scores and metrics\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    \n",
    "    # Print analysis scores\n",
    "    print(f\"Positive Score: {positive_score}, Negative Score: {negative_score}, Polarity Score: {polarity_score}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    sentences = text.split('.')\n",
    "    avg_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "    complex_words = [word for word in words if len(re.findall(r'[aeiou]', word)) > 2]\n",
    "    print(f\"Complex Words: {complex_words[:10]}...\")  # Print first 10 complex words\n",
    "    \n",
    "    # Final output row\n",
    "    output_row = [row['URL_ID']] + analyze_text(text)\n",
    "    print(f\"Output Row for URL_ID {row['URL_ID']}: {output_row}\")\n",
    "    \n",
    "    # Append the row to the output data\n",
    "    output_data.append(output_row)\n",
    "\n",
    "# After processing, print final data preview\n",
    "print(\"First few rows of output data:\")\n",
    "print(output_data[:5])  # Print first 5 rows of output data\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_df = pd.DataFrame(output_data, columns=columns)\n",
    "output_df.to_excel(\"Analyzed_Output.xlsx\", index=False)\n",
    "\n",
    "print(\"Data processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d44dc-260e-4bb6-a163-5a419e89c9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (py36env)",
   "language": "python",
   "name": "py36env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
